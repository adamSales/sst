\documentclass[12pt]{article}

%\usepackage{endfloat}
\usepackage{type1ec}
\usepackage{subcaption}
\usepackage{fullpage}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{multirow}
\usepackage{comment}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{setspace}
\usepackage{verbatim}
\usepackage{natbib}
\usepackage{bm}
\usepackage{pdflscape}
\usepackage{tikz}
\usepackage{xr}
\usepackage{hyperref}

\newcommand{\dalpha}{\hat{d}_\alpha}
\newcommand{\dstar}{d^*}
\newcommand{\ps}{\bm{p}_D}
\newcommand{\dhat}{\hat{d}}
\newcommand{\dhatm}{\hat{d}_M}
\newcommand{\hedged}{H_{0d}^{edge}}
\newcommand{\htotd}{H_{0d}^{tot}}
\newcommand{\EE}{\mathbb{E}}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}


\doublespacing

\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newtheorem{conjecture}{Conjecture}
\newtheorem{ce}{Counter-Example}
%\newtheorem{ass}{Assumption}
\newtheorem{alg}{Algorithm}
%\newtheorem*{ass*}{Assumption}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}

\newenvironment{ass}[2][Assumption:]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2}.]}{\end{trivlist}}

<<include=FALSE>>=
library(knitr)
library(tikzDevice)
opts_chunk$set(
echo=FALSE, results='asis',cache=TRUE,warning=FALSE,error=FALSE,dev='tikz',message=FALSE,autodep = TRUE
    )

options(tikzDefaultEngine = "pdftex")


@

<<functions>>=

dhatm <- function(ps){
    M <- cumsum(ps-1/4)
    which.max(M)
}

dhatAlpha <- function(ps,alpha){
    D <- length(ps)
    max(seq(D)[ps>=alpha],na.rm=TRUE)
}

ds <- function(ps,alphas=c(0.05,0.15,0.25)){
    dhat <- list()
    for(a in alphas){
        dhat[[paste0('alpha',a)]] <- dhatAlpha(ps,a)
    }
    dhat[['Mallik']] <- dhatm(ps)
    dhat
}

@

\title{Sequential Specification Tests to Choose a Model}

\author{Adam Sales}

\begin{document}

\maketitle

\section{Introduction}

\section{The Setup}

Say, in specifying a model, a research must choose from a discrete,
ordered, set of specifications, such as variables to include in a
linear model, bandwidths in a regression discontinuity design, or
calipers in a propensity-score matching design.
Denote the set of possible choices $d=1,2,\dots,D$.

The researcher wishes to choose the best model that satisfies a
particular testable assumption $\mathcal{A}$.
Assume that either $\mathcal{A}$ is false for all $d$, or that for
some $1\le \dstar\le D$, $\mathcal{A}$ is true for $d\le \dstar$ and false
for all $d>\dstar$.
Further assume that if $\dstar$ exists, it is the optimal choice---for
instance, the smallest model, or the biggest dataset, that satisfies
$\mathcal{A}$.
Finally, assume the researcher has chosen a valid, unbiased test of
$\mathcal{A}$ and caluculated p-values for each $d$:
$\bm{p}_D=p_1,\dots,p_d,\dots,p_D$.
The procedure here is to use $\bm{p}_D$ to choose a specification
$\hat{d}$ that is as large as possible without violating
$\mathcal{A}$.


A common choice for $d$ in this scenario relies on the logic of null
hypothesis testing: for a pre-specified $\alpha \in (0,1)$, let
\begin{equation*}
\dalpha \equiv max\{d : p_d>\alpha\}.
\end{equation*}
That is, $\dalpha$ is the largest value of $d$ for which the
null hypothesis that $\mathcal{A}$ is true for $d\le \dalpha$
cannot be rejected at level $\alpha$.
Although it may seem as though the multiplicity of tests invovled in
this procedure invalidates the null hypothesis framework, it turns out
that this is not the case: the ``stepwise intersection-union
pricipal'' \citet{berger1988, rosenbaum2008,hansen2015} insures that the
family-wise error rate is maintained.
That is, the probability of falsely rejecting the null---choosing
$\dalpha<\dstar$, is bounded by $\alpha$.
$\dalpha$ is the specification that would result from testing null
hypotheses backwards: for $d'=D,D-1,\dots,d,\dots,1$, test $H_{0d'}:$
$\mathcal{A}$ is true for $d\le d'$.
Then, stop testing at $d'=\dalpha -1$---the first $d'$ for which
$p_{d'} \ge \alpha$; reject all null hypotheses $H_{0d'}$ for which
$d'\ge \dalpha$, and fail to reject the rest.
This protects the family-wise error rate of $\alpha$ since rejecting
\emph{any} true null implies rejecting the first true null---a
probability $\alpha$ event.

A tempting alternative choice for $\hat{d}$, say $\hat{d}_{\tilde{\alpha}}$,
does not have this property.
The choice $\hat{d}_{\tilde{\alpha}}\equiv min\{d: p_d<\alpha\}-1$
selects $\hat{d}$ to be the largest value of $d$ before the first
significant p-value.
This is equavalent to the opposite procedure: start with the $d'=1$
and test sequentially for larger values of $d'$ until the first
rejection, at $\hat{d}_{\tilde{\alpha}}$, then stop; reject all null
hypotheses $H_{0d'}$ for $d'\ge \hat{d}_{\tilde{\alpha}}$ and fail to
reject the rest.
This procedure does not control family-wise error rates---it is likely
to reject more than $100\alpha$\% valid specifications.

This paper will focus on two data scenarios for SSTs.
In the first, the SSTs help determine which data are included in the
analysis.
For instance, choosing the bandwidth of a regression discontinuity
design, or choosing the parameters of a matching design.
In this scenario, each choice $d$ corresponds to rows in the dataset
that could be included in the analysis.
Formally, let $\mathcal{I}=\{1,\dots,N\}$, indices for $N$ candidate
cases to be fit in a model.
Then let $\mathcal{I}=\mathcal{i}_1\cup\mathcal{i}_2\cup\dots\cup
\mathcal{i}_d \cup \dots union \mathcal{i}_D$.
The choice of $\hat{d}$ means fitting the model to the dataset
including each of these subsets,
$\mathcal{I}_d=\mathcal{i}_1\cup\dots\mathcal{i}_d$.
Note that the sets denoted with lower-case $\mathcal{i}_d$ are disjoint,
$\mathcal{i}_d \cap \mathcal{i}_{d'}=\emptyset$,
those denoted with upper-case $\mathcal{I}_d$ are nested---$d>d'$
implies $\mathcal{I}_{d'}\subset \mathcal{I}_d$, and the full set of
indices, noted without a subscript, $\mathcal{I}=\mathcal{I}_D$.
Finally, let $n_d=|\mathcal{i}_d|$, where $|\cdot |$ denotes
cardinality, and $\bar{n_d}=|\mathcal{I}_d |$.

A second scenario applies when the dataset is fixed, but the model is
not.
Here, $d$ indexes a \emph{pre-specified} sequence of models.
For instance, if SSTs are used to determine which variables should be
included in a model, with the goal being smallest model that satisfies
$\mathcal{A}$.
Then let $\bm{X}$ denote the full set of variables, $\bm{x}_d$ denote
the set of variables that would be \emph{subtracted} in the $d^{th}$ step
of the sequence, and $\bm{X}_d=\bm{x}_{d+1} \cup \dots \cup \bm{x}_D$
denote the set of variables that would be included in the analysis,
were the analyst to choose $d$.
Note here that bigger values of $d$ correspond to smaller models.
In this scenario, the sample size is fixed at $N$.

\subsection{Model Selection and the Logic of Null Hypothesis Testing}
In order to avoid certain methodogical mistakes, it may be helpful to
clarify some of the conceptual distinctions between SSTs and
conventional null hypothesis tests (NHTs).
The logic of NHTs is familiar to anyone who has taken (and understood)
even the most basic college statistics course; nonetheless we restate
it here to distinguish it from the logic of SSTs.
Typically, researchers use NHTs to reject a null hypothesis that they
consider uninteresting---most of the time, that a model parameter is
equal to zero---and interpret rejection as evidence in favor of an
interesting alternative hypothesis.
NHTs cap the probability of a type-I error---falsely rejecting a true
null hypothesis---and, given that constraint, seek to minimize the
probability of a type-II error, failing to reject a false null
hypothesis.

SSTs reverse some of these elements; most importantly, the goal of
SSTs is to identify specifications in which an assumption
$\mathcal{A}$ is plausible, rather than to identify true alternative
hypothesis.
In the same vein, type-II errors are typically of more concern for
SSTs than for typical NHTs, and type-I errors are less problematic.
In fact, a type-II error from a specification test could lead a
researcher to fit a misspecified model, which in turn may inflate the
probability of a type-I error in her final outcome analysis.
For that reason, some methodologists recommend setting $\alpha$
substantially higher for specification tests than for NHTs in outcome
analyses.
Still, the hypothesis testing framework, in the case of pont null
hypotheses, does not allow a researcher to
fix the type-II error rate at a pre-specified value, and then optimize
the type-I error rate, though that might be ideal for
specificaton tests.

In fact, in continuous data models with continuous parameter spaces,
no hypothesis test can provide any evidence in favor of a point null
hypothesis.
For instance, take the common $H_0: \theta=0$, for some parameter
$\theta\in \mathbb{R}$.
In finite samples, for any type-I or type-II error rate, there will
always be some plausible alternative hypothesis $H_a
\theta=\epsilon\ne 0$.
Further, in these situations, finite sample estimates $\hat{\theta}$
will almost surely be non-zero.
This is important to state to avoid misinterpretations of SST
procedures as providing evidence, or showing, that an assumption
$\mathcal{A}$ is true for certain specificatons $d$.
A common Bayesian argument (e.g. \citealp[][p. 439]{kadanePrinciples};
\citealp{gelmanBlog}) states that, theoretically, nearly all null
hypotheses are false anyway---so testing them makes little sense.
In the case of specification tests, that means that an assumption
$\mathcal{A}$ can be assumed to be false for all $d$ without even
conducting a test; in other words, ``all models are wrong''
\citep[p. 2]{modelsWrong}.

``But some are useful.''
In practice there is much to be gained by considering assumptions such
as $\mathcal{A}$.
In this framework, it may indeed make sense to identify a set of
specifications $d$ for which $\mathcal{A}$ is plausible, or
approximately true, and
SSTs can be useful in this regard---as long as they are understood
correctly, and not as providing evidence \emph{for} $\mathcal{A}$.

In many scenarios the choice of $d$ involves a bias-variance tradeoff:
if $d>\dstar$, then $\mathcal{A}$ is false and the resulting analysis
will be biased.
On the other hand, a sub-optimal choice for $d$ often means a
high-variance estimate.
For instance, in the RDD bandwidth case, choosing $d>\dstar$ might mean
fitting a misspecified model to $Y$ and $R$, but choosing $d<<\dstar$
means discarding data that can boost precision.
Rather than choosing a criterion, such as mean-squared-error, that
balances bias and variance, the SST approach may be seen as an attempt to hold
bias at approximately zero, and minimize variance under that
constraint.
Granted, this is obviously an overly-optimistic take on model fitting;
still, SSTs hope to contrain bias to be approximately zero, and from there
minimize variance.


\subsection{More Reservations with Null Hypothesis Testing for Model
  Selection}

Applying a strict hypothesis-testing framework to SSTs for model
selection has some additional drawbacks.
First, it requires researchers to choose a test-level $\alpha$. While
using tuning parameters to mediate the bias-variance tradeoff is not
uncommon in statistics, the level $\alpha$ is a particularly hard
parameter to choose.

\citet{granger} poses an additional problems with the use of
hypothesis tests to choose a model: the need to specify a null
hypothesis. In their words (p. 179),
\begin{quote}
Whenever a hypothesis test is used to choose between two models, one
model must be selected as a null hypothesis. In most instances, this
is usually the more parsimonious model and typically a nested test is
applied. Often it is difficult to distinguish between the two models
because of data quality (multicol- linearity, near-identification, or
the models being very similar such as in testing for integration). In
such cases, the model chosen to be the null hypothesis is unfairly
favored.
\end{quote}
In other words, because of the structure of null hypothesis tests,
which constrain the type-I error rate, the null model is unfairly
favored.
In our terminology, $\hat{d}$ is likely to be too small, perhaps $\EE
\hat{d}<\dstar$.
However, such a bias (if it indeed exists) needn't doom SSTs---an
underestimated $\hat{d}$ is merely sub-optimal. In our setup, choosing
$\hat{d}$ to be too low will yield and inefficient, but still valid,
model.
Would that every statistical model were valid yet suboptimal!

More broadly, perhaps, one might argue that null hypothesis tests are
design to rule out hypotheses that are inconsistant with the data, not
to estimate parameters.
However, as \citet{hodgeslehmann} showed, these aims are not
contradictory---tests that rule out implausible hypothesis may also
point researchers towards the correct answer.

Moving from rejecting implausible specifications to estimating optimal
specifications requires a theory, or at least a reasonable heuristic.
The following section will suggest one.

\section{Finding the Change-Point}
In the context of change point estimation, \citet{mallik} suggests
such a heuristic.
They discuss a random variable $x_t$, whose distribution is a function
of a continuous covariate $t$.
For $t<d^*$, $\EE x_t=\tau_0$, a constant; for $t>\dstar$, $\EE
x_t>\tau_0$.
They propose an estimate of $d_0$ based on p-values $p_t$
testing the hypotheses $H_{0t}:\EE x_t=\tau_0$.
They note that for $t<\dstar$, the null hypotheses are true, so
$p_t\sim U(0,1)$, and $\EE p_t =1/2$; when $t>\dstar$, the null hypotheses are false, and
the p-values converge in probability to zero.
That fact leads them to the following least-squares estimator for
$\dstar$:
\begin{equation*}
\dhatm\equiv argmin_d \displaystyle\sum_{t\le d} (p_t -1/2)^2 +
\displaystyle\sum_{t>d} p_t^2.
\end{equation*}
In other words, the estimate $\dhatm$ is the point at which the
p-values cease behaving as p-values testing a true null, with mean
$1/2$, and instead are drawn from a distribution with a lower mean.
It turns out that an equivalent expression for $\dhatm$ is:
\begin{equation}\label{eq:mallikSimple}
\dhatm=argmax_d \displaystyle\sum_{t\le d} (p_t-1/4).
\end{equation}
\citet{mallik} shows that as $n_t$, the number of data points at each value
$t$, and the number of sampled values of $t$ increase, $\dhatm$
converges in probability to $\dstar$.

The same broad logic applies to any set of p-values from sequential tests:
$\dhatm=argmax_d \sum_{t\le d} (p_t-1/4)$ may be considered an
estimate of $\dstar$.
In the case of SSTs, for $d\le \dstar$, p-values $p_d$ are draws from
a $U(0,1)$ distribution, and hence have mean 1/2, and, as $n_d$ or $N$
increase, $p_d \rightarrow_p 0$ for $d>\dstar$.
Some differences in the details, though, lead to differences in
$\dhatm$'s behavior.
For instance:
\begin{lemma}
If indeed $p_d\rightarrow_p 0$ for $d>\dstar$, as $n_d$ or $N$
increase, then $\dhatm$ is assymptotically conservative:
$Pr(\dhatm>\dstar)\rightarrow 0$.
\end{lemma}
\begin{proof}
For each $d$, $Pr(p_d -1/4>0)\rightarrow 0$, implying that for all $d'$, $Pr(\sum_{\dstar <t\le d'}
(p_t-1/4)>0)\rightarrow 0$.
Therefore, for $\dstar<d\le D$, $Pr(\sum_{t\le d} (p_t-1/4)> \sum_{t\le
  \dstar} (p_t-1/4))\rightarrow 0$.
\end{proof}
That is, as sample size increases, the probability that $\dhatm$
suggests a model that violates assumption $\mathcal{A}$ decreases to
zero.
The same property holds for $\dhat_\alpha$, with $\alpha>0$ fixed, for
the same reason.

On the other hand, even with an infinite sample $\dhatm$ may choose a
sub-optimal model, $\dhatm<\dstar$.
As sample size grows, the distribution of $p_d$, $d\le \dstar$ remains
stable at $U(0,1)$.
When $p_\dstar-1/4<0$, $\dhatm \neq \dstar$, since $\sum_{d\le
  \dstar-1} (p_d-1/4)>\sum_{d\le \dstar} (p_d-1/4)$.
Since $Pr(p_\dstar-1/4<0)=1/4$ regardless of sample size, $\dhatm$
will be conservative in large samples.
The difference between the SST case discussed here and the
change-point case in \citet{mallik} is that the latter case relies on
a continuous covariate that may be sampled from any point on the unit
interval, whereas in the SST case the choice set $d=1,2,\dots,D$ is
discrete and held fixed in the assymptotics.

In a way, $\dhatm$ is similar to $\dhat_{0.25}$, the largest $d$ for
which $p_d>\alpha=0.25$, since both penalize p-values lower than
$0.25$.
However, for a given set of p-values, $\dhatm \le \dhat_{0.25}$.
To see this, note that for all $d>\dhat_{0.25}$, $p_d<0.25$, so every
summand $p_d-1/4$ after $\dhat_{0.25}$ is negative.
Therefore, the maximum of $\sum_{i\le d} (p_i-1/4)$ must occur with $d
\le \dhat_{0.25}$.
While $\dhat_{0.25}$ and $\dhatm$ may often coincide, there are also
cases in which $\dhatm<\dhat_{0.25}$.
This will happen when the maximum value of the random walk in
(\ref{eq:mallik}), occurs prior to $\dhat_{0.25}-1$.
Then, $\dhatm$ will only equal $\dhat_{0.25}$ if
$p_{\dhat_{0.25}}-1/4>max_d\{ \sum_{i\le d} (p_i-1/4)\}-\sum_{i\le
  \dhat_{0.25}-1} (p_i-1/4)$.

In general, the difference between $\dhat_\alpha$ and $\dhatm$ will be
most pronounced when the distributions of p-values for $d>\dstar$ are
not monotonically decreasing in probability---in such a scenario, it
is most probable that an errent p-value for $d>>\dstar$ will be
greater than $\alpha$; one p-value determines $\dhat_\alpha$, but
$\dhatm$ relies on the entire set of p-values.

\section{Edge Testing}
Hypothesis tests will only be able to reject specifications that
violate assumption $\mathcal{A}$ if they have sufficient power.
One simple way to boost power in some scenarios is to focus hypothesis
tests on the parts of the data and model that is most likely to
violate the assumption---the difference between the analysis under $d$
and under $d-1$.

In contrast to specification tests that researchers use to check
fully-specified models, and are designed to check the model as a
whole, SSTs are an explicit and planned part of the model selection
process.
That being the case, their focus should be on differences between
potential specifications, rather than on overall suitability.
We refer to the former as ``edge testing,'' since it focuses
hypothesis tests on edge cases, and the latter ``total testing.''

When decisions $d$ determine which data are included in the analysis,
as in RDD bandwidth selection, the choice between edge and total
testing is a choice between null hypotheses to test.
The edge null is:
\begin{equation}
\hedged: \mathcal{A} \text{is true for } i \in
\mathcal{i}_d
\end{equation}
whereas the total null is
\begin{equation}
\htotd: \mathcal{A} \text{is true for } i \in
\mathcal{I}_d
\end{equation}
where, as above,
$\mathcal{I}_d=\mathcal{i}_1\cup\dots\mathcal{i}_d$, all data
included in specification $d$.

For instance, in selecting a bandwidth for an RDD, as in
\citet{rocio}, researchers test for, say, equality of means of a
covariate $x$ between treated subjects, with running variable values
$R$ at one
side of the cutoff, and control subjects with $R$ on the other side.
Here $d$ indexes candidate bandwidths, $max |R-c|=bw_d$.
Then $\mathcal{i}_d=\{i: |R_i-c|=bw_d\}$ and $\mathcal{I}=\{i:
|R_i-c|\le bw_d\}$.
Therefore, $\htotd : \EE [x| 0<R-c\le bw_d]=\EE [x|-bw_d \le R-c <0]$
and $\hedged :\EE [x|R-c=bw_d]=\EE [x|
R-c=-bw_d]$.\footnote{These are simplifications of Assumption 4 in
  \citet{rocio}, which treats $x$ as fixed, not random.}
For the sake of demonstration, say $var(x)=\sigma^2$.
For $d \le \dstar$, $\EE [x ||R-c|= bw_d]=0$ but for $d>\dstar$
$\EE[x | R-c = bw_d]=\tau$ and $\EE [x|R-c =-bw_d]=-\tau$.
Further, say there are $n_d=n_0$ at each possible bandwidth $bw_d$.
For $d=\dstar+1$, testing $\htotd$ means comparing the means of two
samples of size $(\dstar+1)n_0$ , each with standard deviation
$\sqrt{\sigma^2+\tau^2(1-1/dstar)}$ and with means $\pm \tau/dstar$.
On the other hand, a test of $\hedged$ compares the means of two
smaller samples, each of size $n_0$, with standard deviation $\sigma$
and means $\pm \tau$.
As long as $\dstar>1$, the power of a t-test for $\hedged$ will be greater
than the power for $\htotd$,\footnote{The non-centrality parameter in
  the $\htotd$ test is
  $\frac{2\tau/\dstar}{\sqrt{\sigma^2+\tau^2(1-1/\dstar)}}\frac{\sqrt{(\dstar+1)n_0}}{\sqrt{2}}$
    and the non-centrality parameter in the $\hedged$ test is
    $\frac{2\tau}{\sigma}\frac{\sqrt{n_0}}{\sqrt{2}}$}
better allowing a the SST procedure to distinguish between $\dstar$
and $d$.

The SST case in which a researcher uses SSTs to choose a model
specification is analogous.
For instance, \citet{ivanov} provide several examples of sequential
tests to determine the order of an autoregressive process, and both
edge and total tests are represented.
The ``general to specific'' liklihood ratio test compares the
determinants of estimated innovation covariance
matrices of models assuming order $d$ and $d-1$, respectively.
It tests the null hypothesis $\hedged$ that the order of the process
is $d$ against the alternative that the order is $d-1$.
They also discuss a ``Portmanteau'' Lagrange Multiplier test that
tests $\htotd$, that there is no serial correlation in the residuals
of a VAR($d$) model.
In this case, the difference between edge and total tests lies in the
alternative---an edge test compares model $d$ to model $d+1$, whereas
a total test compares all candidate model to the same alternative.

\section{Two Data Examples}

To illustrate these ideas---edge testing, and the change point and
hypothesis testing approaches to selecting $d$---we will briefly
illustrate them with two data examples.
The two examples correspond to the two broad categories of
specification we have discussed: selecting data to analyze and
selecting a model specification.

\subsection{SSTs in Regression Discontinuity Bandwidth Selection:
  Estimating the Effect of  Academic Probation on College GPAs}

At many universities, students who fail to achieve a minmum GPA are
put on academic probation (AP) \citep[See, e.g.][]{tovar2006academic}.
This provides them access to a set of resources designed to address
personal issues that may be hindering their  performance.
Perhaps more importanty, AP is a threat---students on AP who do not
improve are subject to disciplinary measures such as suspension.
\citet{LSO} recognized that AP can form a regression discontinuity
design (RDD), in which treatment is a function of a ``running
variable'' with a pre-determined cutoff.
Specifically the treatment $Z$, students' AP status, is (almost) a
deterministic function of a ``running variable'' $R$, students'
grade-point-averages (GPAs).
Students with a GPA below a pre-determined cutoff, $R<c$, are put on AP.
That being the case, students with GPAs just below $c$ may be
comparable to students with GPAs just above $c$---comparing these two
sets of students allows researchers to estimate the effect of AP on
outcomes $Y$.
The challange becomes defining ``just above'' and ``just below''; SSTs
may be able to play a role here.

For example, \citet*{rocio} (CFT) suggests directly
comparing the outcomes of subjects with $R$ very close to $c$, say with
$R\in [c-bw,c+bw]$ for some bandwidth $bw>0$
To choose $bw$, CFT uses pre-treatment covariates $\bm{X}$, and
covariate balance tests range of candidate bandwidths.
For each possible $bw$, they test the hypothesis that the covariates
are balanced:
\begin{equation}\label{eq:covBal}
\bm{X}\independent Z| R\in[c-bw,c+bw]
\end{equation}
and choose the largest bandwidth in which (\ref{covBal}) cannot be
rejected.

Bandwidth selection for RDDs, and the role of covariate balance tests,
encompasses a growing literature.
As its name suggests, regression discontinuity typically relies on
regression modeling: the goal is to model $Y$ as a function of $R$ on
either side of $c$ to estimate the average treatment effect for
subjects with $R$ in an infinitesimally-small interval around the
cutoff $c$ \citep[See][]{imbensRD}.
In contrast, CFT dispenses with regression alltogether.
One popular way to ensure robustness to model misspecification is to
fit the regression models to a subset of the data with $R$ in a
window around $c$.
A number of methods exist to choose an optimal bandwidth $bw$---the width
of the window---that is both large enough to allow for precise effect
estimation but small enough to ensure robustness.
\citet{IK} suggest using non-parametric estimates of the curvature of
the regression function of $Y$ on $R$, combined with local linear
regression, to choose a $bw$ that minimizes mean-squared-error.
However, other authors have suggested choosing $bw$ (or an analogous quantity) based on
SSTs, including \citet{mattai}, which presents a Bayesian approach
analogous to CFT's, \citet{salesHansen}, which discusses the use of robust
regression models,  and \citet{angristWanna}, which proposes a method
to estimate effects for subjects with $R$ farther from $c$.
In the latter paper, SSTs do not test covariate balance, but the
irrelevence of $R$ conditional on covariates $X$, for subjects in a
given bandwidth.

This section will illustrate several approaches to SSTs in the context
of estimating the effect of AP for first year college students on
subsequent GPAs.
For the sake of simplicty, the discussion will be limited to CFT's
general approach to regression discontinuity designs; however, many of
the SST methods can be extended to other RDD analyses.
In their analysis, CFT  considered a set of seven covariates:
students' high-school GPA (expressed in percentiles), age at college
metriculation, number of attempted credits, gender, native language
(English or other), birth place (North America or other) and
university campus (the university consisted of three campuses).
A version of Hotellings $T^2$ test that models
treatment assginment $Z$, and not $X$, as random \citep{hansenBowers}
is used to test balance.
The resulting p-values, Total Testing---testing hypotheses
(\ref{eq:covBal})---and Edge Testing, testing $H_{0bw}: X\independent Z
| |R|=bw$, are plotted in Figure \ref{fig:rddpvalues1}.

\begin{figure}
<<rdd,fig.width=6,fig.height=4,results='hide'>>=
source('src/winSelect.r')
source('src/lso.r')

par(mfrow=c(1,2))
plot(bws,psT,main='Total Testing',xlab='$bw$',ylab='p-value',ylim=c(0,1))
wtplus <- windowsT[windowsT>0]
abline(v=bws[unique(wtplus)],col=seq(length(unique(wtplus))))
legend('topright',legend=vapply(unique(wtplus),function(bw)
    do.call('paste', c(as.list(paste0('$',names(wtplus)[wtplus==bw],'$')),sep=',')),'a'),
       col=seq(length(unique(wtplus))),lty=1)

plot(bws,psE,main='Edge Testing',xlab='$bw$',ylab='p-value',ylim=c(0,1))
weplus <- windowsE[windowsE>0]
abline(v=bws[unique(weplus)],col=seq(length(unique(weplus))))
legend('topright',legend=vapply(unique(weplus),function(bw)
    do.call('paste', c(as.list(paste0('$',names(weplus)[weplus==bw],'$')),sep=',')),'a'),
       col=seq(length(unique(weplus))),lty=1)
@
\caption{P-values from total and edge testing for balance in all seven covariates
  from the LSO analysis. Vertical lines denote bandwidth choices using
  different criteria.}
\label{fig:rdpvalues1}
\end{figure}

In this case, total and edge testing paint similar pictures:
covariates are imbalanced for most bandwidths.
Both $\dhat_M^{0.5,0}$ and $\dhat_M^{a,b}$, marked with a green vertical
lines in Figure \ref{fig:rdpvalue1}, select the lowest possible
bandwidth.
Since the first p-value \Sexpr{round(psT[1],3)} is below any $\alpha$
considered, a strategy that chooses the last non-rejected bandwidth
(denoted above as $\hat{d}_{\tilde{\alpha}}$) rejects every bandwidth.
According to these methods, the CFT method is unsuitible for this
dataset.
However, the scattered large p-values at some bandwidths lead versions
of $\dhat_\alpha$ to select larger bandwidthds.
This fact illustrates a weakness of $\dhat_\alpha$: strong evidence
against a model specification will be discarded in the presence of one
favorable p-value.

\begin{figure}
<<rdd2,fig.width=6,fig.height=4,results='hide'>>=

par(mfrow=c(1,2))
plot(bws,psT1,main='Total Testing: High School GPA',xlab='$bw$',ylab='p-value',ylim=c(0,1))
wtplus <- windowsT1[windowsT1>0]
abline(v=bws[unique(wtplus)],col=seq(length(unique(wtplus))))
legend('topright',legend=vapply(unique(wtplus),function(bw)
    do.call('paste', c(as.list(paste0('$',names(wtplus)[wtplus==bw],'$')),sep=',')),'a'),
       col=seq(length(unique(wtplus))),lty=1)

plot(bws,psE1,main='Edge Testing: High School GPA',xlab='$bw$',ylab='p-value',ylim=c(0,1))
weplus <- windowsE1[windowsE1>0]
abline(v=bws[unique(weplus)],col=seq(length(unique(weplus))))
legend('topright',legend=vapply(unique(weplus),function(bw)
    do.call('paste', c(as.list(paste0('$',names(weplus)[weplus==bw],'$')),sep=',')),'a'),
       col=seq(length(unique(weplus))),lty=1)
@
\caption{P-values from total and edge testing for balance in all seven covariates
  from the LSO analysis. Vertical lines denote bandwidth choices using
  different criteria.}
\label{fig:rdpvalues2}
\end{figure}


To better illustrate differences between the window selection
strategies, we consider the covariate high school GPA alone.
Since the outcme of interest is itself a GPA, prior measures of GPA
are arguably the most relevant and important to control.
P-values from total and edge tests of balance in high school GPA are
displayed in Figure \ref{fig:rdpvalues2}.
Fortunately for the illustration here, high school GPA may be
balanced for small $bw$s.
P-values from total and edge testing in \ref{fig:rdpvalues2} are
markedly different.
Edge testing shows some bandwiths, considered in
isolation, appear to plausibly satisfy covariate balance, while others do not.
On the other hand, the p-values from total testing are more nearly
monotonic: imbalance in high school GPA at lower bandwidths causes
balance tests to reject at higher bandwidths as well.
The opposite also occurs: for instance, at bandwidth
$\Sexpr{bws[16]}$, total testing gives \Sexpr{round(psT1[16],3)},
whereas edge testing gives \Sexpr{round(psE1[16],3)}.
The explanation is that high school GPA at that bandwidth is indeed
imbalanced, but in the opposite direction of other imbalances: the AP
students at $bw=$\Sexpr{bws[16]} had \emph{higher} high school GPAs
than those who were not on AP, to an extent that counteracted
imbalances at smaller bandwidths.

That said, edge testing suggested larger bandwidths for almost all
procedures.

% For $\dhat_\alpha$ was moderate for total testing,
% \Sexpr{bws[windowsT1[1]]} for $\alpha=$0.05 and
% \Sexpr{bws[windowsT1[2]]} for $\alpha=$0.15 and 0.25,
% and large for edge testing: \Sexpr{bws[windowsE1[1]]} for all three
% values of $\alpha$, again illustrating the sensitivity of
% $\dhat_\alpha$ to outlier p-values.
% This is especially worrying for edge testing, in which small samples
% for some values of $d$ can almost certainly ensure high p-values.
% In this case, there were \Sexp{sum(ap[['r']]==bws[windowsE1[1]])}
% control students and \Sexp{sum(ap[['r']]==-bws[windowsE1[1]])} AP
% students at a bandwidth of $bw=$\Sexpr{bws[windowsE1[1]]}.

% The change-point approach to window selection is less sensitive, since
% it accounts for p-values at every bandwidth simultaneously.
% The assymptotic change-point estimate, $\dhat_M^{0.5,0}$ chose
% $bw=$\Sexpr{bws[windowsT1[4]]} with total testing
% and \Sexpr{bws[windowsE1[4]]} with edge testing.
% The finite sample version chose
% $\dhat_M^{a,b}=$\Sexpr{bws[windowsT1[5]} for total
% testing and \Sexpr{bws[windowsE1[5]]} for edge testing.
% These estimates can capitalize on the strengths of edge testing, in
% some circumstances, without necessariy succumbing to their weaknesses,
% since they are not driven by individual p-values but by the entire
% distribution.
This example suggests that edge testing might only be suitible for
change-point-based window selectors.
Futher, the more flexible $\dhat_M^{a,b}$ outperformed the
assymptotic estimator $\dhat_M^{0.5,1}$ by choosing
$b=$\Sexpr{dhatm2['b']}, so that even moderately small p-values
suggested departures from covariate balance.

\subsection{Lag Order in AR($\rho$) Models: US Total Unemployment}


<<var>>=
source('src/timeSeries2.r')

par(mfrow=c(1,2))
plot(pvals2,pch=16,xlab='Lag Order',ylab='LR Test p-value',main='Edge Tests')
abline(v=dhatsEdge,col=seq(length(unique(dhatsEdge))))
legend('topleft',legend=vapply(unique(dhatsEdge),function(bw)
    do.call('paste', c(as.list(paste0('$',names(dhatsEdge)[dhatsEdge==bw],'$')),sep=',')),'a'),
       col=seq(length(unique(dhatsEdge))),lty=1)

plot(pvalsFull2,pch=16,xlab='Lag Order',ylab='LR Test p-value',main='Tot Tests')
abline(v=dhatsTot,col=seq(length(unique(dhatsTot))))
legend('topleft',legend=vapply(unique(dhatsTot),function(bw)
    do.call('paste', c(as.list(paste0('$',names(dhatsTot)[dhatsTot==bw],'$')),sep=',')),'a'),
       col=seq(length(unique(dhatsTot))),lty=1)
@
\caption{P-values from likelihood ratio tests to select a lag order in
  the unemployment data.}
\label{fig:rdpvalues1}
\end{figure}



par(mfrow=c(1,2))



@



\section{A Simulation Study}


\section{Discussion}

\end{document}