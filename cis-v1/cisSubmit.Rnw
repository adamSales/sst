\documentclass[12pt]{article}

\usepackage[tablesfirst]{endfloat}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsfonts,amsthm}
\usepackage{setspace}
\usepackage{multirow}
\usepackage{natbib}
\usepackage{bm}
\usepackage{booktabs}

\usepackage{array}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}

\usepackage{url}

\newcommand{\dalphaU}{d^{max}_\alpha}
\newcommand{\dalphaB}{d^{min}_\alpha}
\newcommand{\dstar}{d^*}
\newcommand{\ps}{\bm{p}_D}
\newcommand{\dhat}{\hat{d}}
\newcommand{\dhatU}{d^{max}}
\newcommand{\dhatB}{d^{min}}
\newcommand{\dhatm}{\hat{d}_M}
\newcommand{\dhatmab}{\hat{d}^{ab}_M}
\newcommand{\EE}{E}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\newtheorem{prop}{Proposition}


\newenvironment{ass}[2][Assumption:]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries
    #2}.]}{\end{trivlist}}




<<include=FALSE>>=
require(ggplot2)
library(knitr)
library(tikzDevice)
library(xtable)
opts_chunk$set(
echo=FALSE, results='tex',cache=TRUE,warning=FALSE,error=FALSE,dev='tikz',message=FALSE,autodep = TRUE
    )
library(kableExtra)
options(tikzDefaultEngine = "pdftex")



@

<<functions>>=

dhatAlpha <- function(ps,alpha){
    D <- length(ps)
    res <- try(max(seq(D)[ps>=alpha],na.rm=TRUE))
    if(class(res)=='try-error') return(NA)
    res
}


dhatM <- function(ps){
    M <- cumsum(ps-0.25)
    res <- which.max(M)
    if(class(res)=='try-error') return(NA)
    res
}

m2 <- function(ps,a,b,d,D)
    sum((ps[1:d]-a)^2)+sum((ps[(d+1):D]-b)^2)

M2 <- function(ps,a,b,D){
    mm <- vapply(seq(D),function(d) m2(ps,a,b,d,D),1)
    c(min(mm),which.min(mm),a,b)
}

dhatM2 <- function(ps,stepsize=0.01){
    D <- length(ps)-1 ### the last value shouldn't be computed--will give NA

    ## minimum p-value given a, over b. b<a, a>=0.5
    mina <- function(a){
        mma <- vapply(seq(0,a-stepsize,by=stepsize),function(b) M2(ps,a,b,D),numeric(4))
        mma[,which.min(mma[1,])]
    }

    mmb <- vapply(seq(0.5,1,by=stepsize),mina,numeric(4))
    setNames(mmb[,which.min(mmb[1,])],c('Mopt','dhat','aopt','bopt'))
}



dhatAlphaBad <- function(ps,alpha){
    D <- length(ps)
    res <- try(min(seq(D)[ps<alpha])-1)
    if(class(res)=='try-error') return(NA)
    res
}

dhatAll <- function(ps, alphas=c(0.05,0.25),backwards=FALSE){
    if(backwards) ps <- ps[length(ps):1]
    res <- setNames(
        c(vapply(alphas, function(alph) dhatAlpha(ps,alph),1),
          vapply(alphas, function(alph) dhatAlphaBad(ps,alph),1),
          dhatM(ps),
          dhatM2(ps)[2]),
        c(paste0('$\\dhatU_{',alphas,'}$'),
          paste0('$\\dhatB_{',alphas,'}$'),
          '$\\dhatm$','$\\dhatmab$')
        )
    if(backwards) res <- vapply(res,function(x) length(ps)-x+1,1)
    res
}

ds <- function(ps,alphas=c(0.05,0.15,0.25)){
    dhat <- list()
    for(a in alphas){
        dhat[[paste0('alpha',a)]] <- dhatAlpha(ps,a)
    }
    dhat[['Mallik']] <- dhatM(ps)
    dhat
}


multLine <- function(dhats,amount=0.1){
    ttt <- table(dhats)
    if(max(ttt)==1) return(dhats)
    for(d in unique(dhats)){
        num <- ttt[paste(d)]
        if(num>1)
            dhats[dhats==d] <- dhats[dhats==d]+seq(-floor(num/2)*amount,by=amount,length=num)
    }
    dhats
}

cols <- colors()[c(red=552,lightblue=636,mag=642,green=255,orange=499,black=153,purple=469,darkblue=477)]
cbbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")


smoothplot <- function(x,y,...){


    naFlag <- is.na(x) | is.na(y)
    x <- x[!naFlag]
    y <- y[!naFlag]


    tmp <- vapply(unique(x), function(xval) c(
        av=mean(y[x==xval],na.rm=TRUE),
        sizes = length(y[x==xval])
        ),numeric(2))

    tmp['sizes',] <- 2*tmp['sizes',]/max(tmp['sizes',])
    plot(unique(x),tmp['av',],cex=tmp['sizes',],...)
    abline(v=0,lty='dotted')


}


facet2 <- function(st,curv){
    ns <- sort(unique(vapply(strsplit(names(st),'_'),function(x) as.numeric(x[2]),1)))
    dat <- NULL
    cname <- ifelse(curv,'curved','mono')
    for(n in ns){

        run <- do.call('rbind',lapply(st[[paste0(cname,'_',n)]],function(x) x$full))
        st1 <- do.call('c',as.data.frame(run))
        st1 <- data.frame(est=st1,method=rep(colnames(run),each=nrow(run)))
        st1$selector <- ifelse(grepl('max',st1$method),'$\\bar{d}_\\alpha$',
                               ifelse(grepl('min',st1$method),'$\\tilde{d}_\\alpha$','$\\hat{d}_M$'))
        st1$method <- gsub('\\\\\\hat\\{d\\}\\^\\{m[axin]*\\}_\\{','$\\\\\\alpha=',st1$method)
        st1$method <- gsub('\\}','$',st1$method)
        st1$method <- gsub('\\\\\\hat\\{d\\$\\^\\{flex\\$_M','a,b',st1$method)
        st1$method <- gsub('\\\\\\hat\\{d\\$_M','0.5,1',st1$method)

        st1$method <- factor(st1$method)
        st1$n <- n

        dat <- rbind(dat,st1)
    }

    dat$n <- factor(dat$n)
    levels(dat$n)=paste0('n=',levels(dat$n))

    p <- ggplot(dat, aes(x=selector, y=est,fill=method)) +
        geom_violin(bw=1,position=position_dodge(.5))+
        geom_boxplot(width=0.1,position=position_dodge(.5),outlier.size=.5)+
        geom_hline(yintercept=10,lty=2)+
            labs(title=paste('Imbalance:',ifelse(curv,'Sinusoidal','Linear')),y='$\\hat{d}$',x='')+
                facet_wrap(~ n,ncol=3)
    p
}

sumStats <- function(st){
    plotlist <- list()
    for(curv in c('mono','curved'))
        for(n in c(10,50,100)){
            run <- do.call('rbind',lapply(st[[paste0(curv,'_',n)]],function(x) x$full))
            plotlist[[paste0(curv,n)]] <- cbind(
                lessThan3=apply(run,2,function(x) mean(x[is.finite(x)]<=3,na.rm=TRUE)),
                moreThan13=apply(run,2,function(x) mean(x[is.finite(x)]>13,na.rm=TRUE)),
                avg=apply(run,2,function(x) mean(x[is.finite(x)],na.rm=TRUE)),
                mse=apply(run,2,function(x) mean((x[is.finite(x)]-10)^2,na.rm=TRUE)),
                sd=apply(run,2,function(x) sd(x[is.finite(x)],na.rm=TRUE)))

        }
    plotlist
}

@

<<lsoAnalysis,cache=TRUE>>=
require(RItools)
require(foreign)
require(sandwich)
require(lmtest)
require(robustbase)

source('../src/lrdFunctions.r')

balTests <- function(dat,bws){
  balMult.control=list(method='sh',reduced.covars=FALSE)
  stopifnot(is.list(balMult.control),
            !any(c('dat', 'BW') %in% names(balMult.control)),
            is.numeric(bws) && all(bws >=0),
            length(bws)==1 || !is.unsorted(bws))
  short <- function(w) {
    nbargs <- c(list(dat=dat, BW=w), balMult.control)
    try(do.call(balMult, nbargs))
  }

  data.frame(bws=bws,ps=vapply(bws,short,1))
}


ap <- read.dta('../data/AEJApp2008-0202_data/data_for_analysis.dta')
#### downloaded 1/2/17 from https://www.aeaweb.org/aej/app/data/2008-0202_data.zip

ap$R <- round(ap$dist_from_cut,2)
ap <- ap[ap$R!=0,]
ap <- ap[abs(ap$R)<1.5,]
ap$R <- round(ap$R+0.005,3)

ap$Z <- as.numeric(ap$R<0)

ap$lhsgrade_pct <- log(ap$hsgrade_pct)

bws <- sort(unique(abs(ap$R)))[-c(1,2)]

pvalsRD <- balTests(ap,bws)

windowsT <- dhatAll(pvalsRD$ps,alphas=c(0.05,0.25))

ates <- setNames(
    lapply(pvalsRD$bws[windowsT],
           function(bw) sh(ap,bw,'nextGPA')),
    names(windowsT[windowsT>0])
)



@

<<lsoIK,cache=TRUE,include=FALSE>>=
ik <- rdd::RDestimate(nextGPA~R,data=ap,cutpoint=0,kernel='rectangular')
@

<<timeSeriesAnalysis,cache=TRUE>>=

library(urca)
data(npext)
#npext <- read.csv('../data/npext.csv')
y <- npext$unemploy[31:nrow(npext)]
y2 <- data.frame(Year=1890:1988,unemp=y)

new <- read.csv('../data/unemployment2.csv',header=F)
### new was downloaded 1/25/17 from BLS/CPS: https://www.bls.gov/cps/cpsaat01.xlsx


new <- with(new,data.frame(year=V1,unemp=log(V16)))
y <- c(y,new$unemp[new$year>1988])
y <- ts(y,start=1890)

lrTest <- function(p,edge=TRUE){
    stopifnot(p<20)
    p <- p+1
    big <- ifelse(edge,p+1,21)
    llBig <- logLik(arp[[big]])
    llSmall <- logLik(arp[[p]])
    lrtest <- as.numeric(2*(llBig-llSmall))
    pchisq(lrtest , df = 1, lower.tail = FALSE)
}


arp <- lapply(0:20,function(p) arima(y,order=c(p,0,0)))
pvalsTS <- vapply(0:19,lrTest,1)
                                        #pvalsFull <- vapply(0:19,lrTest,1,edge=FALSE)

aics <- vapply(arp,AIC,1)
bics <- vapply(arp,BIC,1)

npTS <- length(pvalsTS)
dhatsTS <- c(dhatAll(pvalsTS,backwards=TRUE,alphas=c(0.05,0.25)),AIC=which.min(aics),BIC=which.min(bics))

dhatsTSnm <- setNames(dhatsTS,c('alphaU05','alphaU25','alphaB05','alphaB25','dm','dmab','AIC','BIC'))



@


\doublespacing


\title{Sequential Specification Tests to Choose a Model: A Change-Point Approach}
\date{}
\begin{document}
\SweaveOpts{concordance=TRUE}
\maketitle
\section{Introduction}

Null hypothesis tests and p-values play a central role in model checking.
In this context, the null hypothesis may be that that the data are
drawn from a distribution contained in the the model under study, or
it may be derived from an underlying assumption.
Typically, researchers use these specification tests to check the fit
of a model chosen by other means, but in some cases hypothesis tests
form the basis of a model selection procedure.
In these cases, researchers construct a sequence of model
specifications, ordered by preferability, and test each one.
The best model whose assumptions ``pass'' the hypothesis test is chosen.

For example, take the datasets displayed in Figure \ref{fig:example},
which will be discussed in more detail in Section \ref{sec:examples}.
Figure \ref{fig:example}A shows the annual total unemployment rate in the
United States from 1890 to 2015.
One of the simpler models for time series such as these is an order
$p$ autoregression, or $AR(p)$, under which the value of the time
series at point $t$ may depend on its historical values at
$t-1,...,t-p$ but, conditional on those, is independent of values at
points before $t-p$.
To choose the order $p$, researchers may test model fit for a
sequence of lag orders $p$, and choose the smallest $p$ that the tests
fail to reject.
Here a smaller lag orders $p$ are preferable because they lead to more
parsimonious models and more precise estimates.

Figure \ref{fig:example}B plots data that \citet{lso} used to estimate the effect of academic
probation on college students' subsequent grade point averages.
University students were put on academic
probation if their first-year cumulative grade point averages
fell below a cutoff.
This is an example of a regression discontinuity design \citep[RDD;][]{thistlewhiteCampbell}, in
which treatment is assigned if a
numeric ``running variable'' $R$ falls below (or above) a
pre-specified cutoff $c$.
Since treatment assignment is entirely a function of $R$ and $c$,
researchers can model the relationship between $R$ and an outcome
variable $Y$ in
order to estimate the effect of the treatment without confounding.
A common tool for ensuring that RDD models are well specified is to
limit the data analysis sample to subjects with $R\in\{c-bw,c+bw\}$,
where $bw>0$ is a bandwidth selected by the data analyst.
One method for choosing $bw$ relies on subjects' baseline
covariates: researchers will estimate ``effects'' of the treatment on
baseline covariates using data from subjects with $R$ within $bw$ of $c$.
Since the treatment cannot possibly have an effect on baseline
covariates, any estimated effects are due to model misspecification or
an overly-large choice of $bw$.
Following this reasoning, some methodologists recommend testing for
effects on covariates using an array of candidate bandwidths, and
choosing the largest bandwidth within which the null hypothesis of no
effect cannot be rejected.
The bandwidth tradeoff is similar to the $AR(p)$ case: if $bw$ is too
large, the causal model might be misspecified and the effect estimate
will be biased. If $bw$ is too small, there will not be enough data to
precisely estimate the effect of interest.

These are both examples of the use of sequential specification tests
(SSTs) to choose a model.
SSTs are also used in covariate selection for regression models
\citep{greene2003econometric}, selecting the number of components in mixture models,
latent class analysis, and factor analysis \citep{nylund2007deciding} and in
propensity-score matching \citep{hansen2015}.

Do hypothesis tests make any sense in model selection?
The results of a null hypothesis test, of course, are never evidence
in favor of a null hypothesis; null hypotheses can only be rejected,
not accepted.
Along similar lines, the logic of controlling type-I error rates seems backwards when it comes to
model selection, in which accepting a
problematic specification---a type II error---is the major concern.
These issues have prompted some methodologists \citep[e.g.][]{cft} to
propose adjusting the size of specification tests to a value higher
than the conventional $\alpha=0.05$.
However, the appropriate value for $\alpha$, and the
criteria for selecting $\alpha$, remain unclear.

On the other hand, a conceptually-sound model-selection method based
on SSTs would be particularly useful;
specification tests already exist for most common models, and
they are regularly taught in introductory quantitative methods
classes.


\begin{figure}
<<examplePlots,fig.width=6,fig.height=4,dev='pdf'>>=

par(mfrow=c(1,2))

plot(y,xlab='Year',ylab='Log Unemployment Rate',sub='(B)')

smoothplot(ap$dist_from_cut,ap$nextGPA,xlab='First-Year GPA\n(centered at cutoff)',ylab='Avg Subsequent GPA',sub='(A)')
#modTrt <- lm(nextGPA~dist_from_cut,data=ap[ap$dist_from_cut<0,])
#modCtrl <- lm(nextGPA~dist_from_cut,data=ap[ap$dist_from_cut>=0,])
#lines(x=c(0,min(ap$dist_from_cut)),y=c(coef(modTrt)[1],coef(modTrt)[1]+coef(modTrt)[2]*min(ap$dist_from_cut)),
 #         xlim=c(min(ap$dist_from_cut),0),col='red')
#lines(x=c(0,max(ap$dist_from_cut)),y=c(coef(modCtrl)[1],coef(modCtrl)[1]+coef(modCtrl)[2]*max(ap$dist_from_cut)),
#          xlim=c(0,max(ap$dist_from_cut)),col='blue')
abline(v=0,lty='dotted')
@
\caption{Plot (A) shows a
  time-series of log annual United States total unemployment from 1890 to
  2015. Data were combined from \citet{urca} and \citet{cps}. Plot (B) shows data from
  \citet{lso}: average subsequent grade point averages (GPAs), as a function of first-year
  GPAs,
  centered at the academic probation cutoff (dotted line).%, with
  %least-squares regression fits.
  The points are sized proportionally to the number of students
  with each first-year GPA.}
\label{fig:example}
\end{figure}
%%% NOTE: schotman & van dijk used AR(4) for unemployment


This paper develops such a method, based on a clever idea in change-point or threshold
estimation.
\citet{mallik} points out that in a process with a change point,
the p-values from a sequence of tests of a null regression function
are uniformly-distributed as long as the regression function is
correct, but asymptotically zero when the function is not correct.
They use this dichotomous behavior to construct a simple, consistent
estimator of the change-point, that is, the point at which the null model
stops being correct.

In the same way, their estimator can choose the change-point in
a sequence of models, when models stop being correct.
Unlike under current approaches, an individual outlier test result will not
drive the change-point estimator, which is instead based on the entire
sequence of p-values.
%% Thus, the change-point view of model selection is arguably conceptually
%% more satisfying and practically more reliable than the conventional
%% test-based approach.
What's more, unlike other SST model selectors, the change-point
approach does not require the researcher to specify a level $\alpha$
or any other tuning parameter.
This approach shifts the model selection rationale away
from the logic of hypothesis testing, based on type-I and type-II
error rates, and towards the logic of estimation.


\section{The Setup, in General}\label{sec:setup}
\subsection{Sequences of Models and Tests}
Say, in specifying a model, a researcher must choose from a discrete,
ordered, set of specifications $d=1,2,\dots,D$.
The resulting model must satisfy testable assumption $\mathcal{A}$.
Assume that either $\mathcal{A}$ is false for all $d$, or that for
some $1\le \dstar\le D$, $\mathcal{A}$ is true for $d\le \dstar$ and false
for all $d>\dstar$.
Further assume that if $\dstar$ exists, it is the optimal choice; for
instance, it is the smallest model, or the biggest dataset, that satisfies
$\mathcal{A}$.
Finally, assume the researcher has chosen a valid, unbiased test of
$\mathcal{A}$ and calculated p-values for each $d$:
$\bm{p}_D=p_1,\dots,p_d,\dots,p_D$.
The goal here is to use $\bm{p}_D$ to choose a specification
$\hat{d}$ that is as large as possible without violating
$\mathcal{A}$.
For the methods we describe here and in the following section, it is
not necessary for the p-values $\bm{p}_D$ to be mutually independent;
indeed, they typically are not.

A common choice for $d$ in this scenario relies on the logic of null
hypothesis testing: for a pre-specified $\alpha \in (0,1)$, let
\begin{equation*}
\dalphaU \equiv max\{d : p_d>\alpha\}.
\end{equation*}
That is, $\dalphaU$ is the largest value of $d$ for which the
null hypothesis that $\mathcal{A}$ is true for $d\le \dalphaU$
cannot be rejected at level $\alpha$.
Although it may seem as though multiplicity corrections may be
necessary here, it turns out
that this is not the case: the ``stepwise intersection-union
principle'' \citep{berger1988, rosenbaum2008,hansen2015} insures that the
family-wise error rate is maintained.
That is, the probability of falsely rejecting the null and choosing
$\dalphaU<\dstar$, is bounded by $\alpha$.
$\dalphaU$ is the specification that would result from testing null
hypotheses backwards: for $d'=D,D-1,\dots,d,\dots,1$, test $H_{0d'}:$
$\mathcal{A}$ is true for $d\le d'$.
Then, stop testing at $d'=\dalphaU -1$, the first $d'$ for which
$p_{d'} \ge \alpha$; reject all null hypotheses $H_{0d'}$ for which
$d'\ge \dalphaU$, and fail to reject the rest.
This protects the family-wise error rate of $\alpha$ because rejecting
any true null implies rejecting the first true null, a
probability $\alpha$ event.

Another common choice for $\hat{d}$  \citep[e.g.][]{lutkepohl2005new}
does not have this property.
Let
\begin{equation*}
\dalphaB\equiv min\{d: p_d<\alpha\}-1
\end{equation*}
$\dalphaB$ selects $\hat{d}$ to be the largest value of $d$ before the first
significant p-value.
This is equivalent to the opposite procedure as $\dalphaU$: start with the $d'=1$
and test sequentially for larger values of $d'$ until the first
rejection, at $\dalphaB$, then stop; reject all null
hypotheses $H_{0d'}$ for $d'\ge \dalphaB$ and fail to
reject the rest.
This procedure does not control family-wise error rates, so it is likely
to reject more than $100\alpha$\% valid specifications.

\section{P-values for Estimation}\label{sec:change-point}

Typically, researchers use hypothesis tests to reject a null hypothesis that they
consider uninteresting, such as a model parameter equal to zero, and
interpret rejection as evidence in favor of an
interesting alternative hypothesis.
Null hypothesis tests cap the probability of a type-I error, and, given that constraint, seek to minimize the
probability of a type-II error.

Sequential specification tests reverse some of these elements; most importantly, their
is to identify specifications in which an assumption
$\mathcal{A}$ is plausible, rather than to identify a true alternative
hypothesis.
In the same vein, type-II errors are typically of more concern for
sequential specification tests than for typical null hypothesis tests,
and type-I errors are somewhat less problematic.
For that reason, some methodologists recommend setting $\alpha$
substantially higher for specification tests than for tests in outcome
analyses.
Still, the hypothesis testing framework, in the case of point null
hypotheses, does not allow a researcher to
fix the type-II error rate at a pre-specified value, and then optimize
the type-I error rate, though that might be ideal for
specification tests.
%% In fact, in continuous data models with continuous parameter spaces,
%% no hypothesis test can provide any evidence in favor of a point null
%% hypothesis.
%% A common Bayesian argument (e.g. \citealp[][p. 439]{kadanePrinciples};
%% \citealp{gelmanBlog}) states that, theoretically, nearly all null
%% hypotheses are false anyway, so testing them makes little sense.
%% In the case of specification tests, that means that an assumption
%% $\mathcal{A}$ can be assumed to be false for all $d$ without even
%% conducting a test; in other words, ``all models are wrong''
%% \citep[p. 2]{modelsWrong}.
%% That said, it may make sense to identify a set of
%% specifications $d$ for which $\mathcal{A}$ is plausible, or
%% approximately true, and
%% sequential specification tests can be useful in this regard.

%% In many scenarios the choice of $d$ involves a bias-variance trade-off:
%% if $d>\dstar$, then $\mathcal{A}$ is false and the resulting analysis
%% will be biased.
%% On the other hand, a sub-optimal choice for $d$ often means a
%% high-variance estimate.
%% For instance, in the regression discontinuity bandwidth case, choosing $d>\dstar$ might mean
%% fitting a misspecified model to $Y$ and $R$, but choosing $d<<\dstar$
%% means discarding data that can boost precision.
%% Rather than choosing a criterion, such as mean-squared-error, that
%% balances bias and variance, the specification testing approach attempts to hold
%% bias at approximately zero, and minimize variance under that
%% constraint.
%% However, doing so requires researchers to choose a test-level
%% $\alpha$.
%% While using tuning parameters to mediate the bias-variance trade-off is not
%% uncommon in statistics, the level $\alpha$ is a particularly hard
%% parameter to choose.

More broadly, one might argue that null hypothesis tests are
designed to rule out hypotheses that are inconsistent with the data, not
to estimate parameters.
However, as \citet{hodgeslehmann} showed, these aims are not
contradictory: tests that rule out implausible hypothesis may also
point researchers towards the correct answer.
Moving from rejecting implausible specifications to estimating optimal
specifications requires a theory, or at least a reasonable heuristic.
%The following section will suggest one.


\subsection{The Change Point Estimator}
In the context of change point estimation, \citet{mallik} suggests
such a heuristic.
They discuss a random variable $x_t$, whose distribution is a function
of a continuous covariate $t$.
For $t<d^*$, $\EE (x_t)=\tau_0$, a constant; for $t>\dstar$, $\EE
(x_t)>\tau_0$.
They propose an estimate of $d_0$ based on p-values $p_t$
testing the hypotheses $H_{0t}:\EE (x_t)=\tau_0$.
They note that for $t<\dstar$, the null hypotheses are true, so
$p_t\sim U(0,1)$, and $\EE (p_t) =1/2$; when $t>\dstar$, the null hypotheses are false, and
the p-values converge in probability to zero.
That fact leads them to the following least-squares estimator for
$\dstar$:
\begin{equation*}
\dhatm\equiv arg\displaystyle\min_{d} \displaystyle\sum_{t\le d} (p_t -1/2)^2 +
\displaystyle\sum_{t>d} p_t^2.
\end{equation*}
In other words, the estimate $\dhatm$ is the point at which the
p-values cease behaving as p-values testing a true null, with mean
$1/2$, and instead are drawn from a distribution with a lower mean.
It turns out that an equivalent expression for $\dhatm$ is:
\begin{equation}\label{eq:dhatm}
\dhatm=arg\displaystyle\max_d \displaystyle\sum_{t\le d} (p_t-1/4).
\end{equation}
\citet{mallik} shows that as $n_t$, the number of data points at each value
$t$, and the number of sampled values of $t$ increase, $\dhatm$
converges in probability to $\dstar$.

The same broad logic applies to p-values from sequential specification
tests.
Some differences in the details, though, lead to differences in
$\dhatm$'s behavior.
For instance:
\begin{prop}\label{prop:conservative}
If indeed $p_d\rightarrow_p 0$ for $d>\dstar$, as $n\rightarrow\infty$ then $\dhatm$ is asymptotically conservative:
$pr(\dhatm>\dstar)\rightarrow 0$.
\end{prop}
\begin{proof}
For each $d>\dstar$, $pr(p_d -1/4>0)\rightarrow 0$, implying that for all $d'$, $pr(\sum_{\dstar <t\le d'}
(p_t-1/4)>0)\rightarrow 0$.
Therefore, for $\dstar<d\le D$, $pr(\sum_{t\le d} (p_t-1/4)> \sum_{t\le
  \dstar} (p_t-1/4))\rightarrow 0$.
\end{proof}
That is, as sample size increases, the probability that $\dhatm$
suggests a model that violates assumption $\mathcal{A}$ decreases to
zero.
The same property holds for $\dalphaU$, with $\alpha>0$ fixed, for
the same reason.

On the other hand, even with an infinite sample $\dhatm$ may choose a
sub-optimal model, $\dhatm<\dstar$.
As sample size grows, the distribution of $p_d$, $d\le \dstar$ remains
stable at $U(0,1)$.
When $p_{\dstar}-1/4<0$, $\dhatm \neq \dstar$, because $\sum_{d\le
  \dstar-1} (p_d-1/4)>\sum_{d\le \dstar} (p_d-1/4)$.
Because $pr(p_{\dstar}-1/4<0)=1/4$ regardless of sample size, $\dhatm$
will be conservative in large samples.
The difference between sequential specification tests and the
change-point case in \citet{mallik} is that the latter case relies on
a continuous covariate that may be sampled from any point on the unit
interval, whereas in the SST case the choice set $d=1,\dots,D$ is
discrete and held fixed in the asymptotics.

In a way, $\dhatm$ is similar to $\dhatU_{0.25}$, the largest $d$ for
which $p_d>\alpha=0.25$, because both penalize p-values lower than
$0.25$.
However, they are not equivalent, as the following proposition shows:
\begin{prop}\label{prop:d25}
$\dhatm \le \dhatU_{0.25}$, with $pr(\dhatm < \dhatU_{0.25})>0$.
\end{prop}
\begin{proof}
By definition, $p_d<0.25$ for all $d>\dhatU_{0.25}$. Therefore,
$\sum_{t=\dhatU_{0.25}+1}^{d'}(p_t-1/4)<0$ for all $d'\ge
\dhatU_{0.25}+1$, which in turn implies that $\sum_{t\le
  \dhatU_{0.25}}(p_t-1/4)>\sum_{t\le d'}(p_t-1/4)$, proving that
$\dhatm\le \dhatU_{0.25}$. On the other hand, if, say,
$p_{\dhatU_{0.25}-1}+p_{\dhatU_{0.25}}<1/2$, or, more generally,
$\sum_{t=d'}^{\dhatU_{0.25}}(p_t-1/4)<0$, then $\dhatm<\dhatU_{0.25}$.
\end{proof}

In general, the difference between $\dalphaU$ and $\dhatm$ will be
most pronounced when the distributions of p-values for $d>\dstar$ are
not monotonically decreasing in probability. In such a scenario, it
is most probable that an errant p-value for $d>>\dstar$ will be
greater than $\alpha$; one p-value determines $\dalphaU$, but
$\dhatm$ relies on the entire set of p-values.

\subsection{A More Flexible $\dhatm$}
In finite samples, p-values from tests of false null hypotheses will
not always be zero.
Similarly, many hypothesis tests are asymptotic and may not yield
uniformly-distributed p-values in finite samples.
Still, p-values from sequential specification tests may exhibit something similar to the
dichotomous behavior that motivates $\dhatm$, in which p-values for
$d\le \dstar$ are distributed differently than p-values for
$d>\dstar$.
For this reason, \citet{mallik} suggested a more flexible estimate:
\begin{equation*}
  \dhatmab \equiv arg\displaystyle\min_{d; 0<b<a<1}
  \displaystyle\sum_{t\le d} ( p_t
  -a)^2+\displaystyle\sum_{t>d} (p_t-b)^2
\end{equation*}
Like $\dhatm$, model selector $\dhatmab$ looks for behavior that
differs between p-values testing true and false null hypotheses.
Unlike $\dhatm$, it does not depend on theoretically established
distributions for these p-values, but searches over a grid for their
location parameters.
$\dhatmab$ will be more computationally expensive to compute than
$\dhatm$, but will may yield better results, especially in small
samples.



\section{A Simulation Study}\label{sec:simulation}
<<simulation,cache=TRUE>>=
source('../src/simpleSim.r')
B <- 10000
res <- sim(B)
NP <- 10
CP <- 5
ppp <- proc(res)
@

This section will present a simulation study to compare the
behavior of model selectors $\dalphaU$, $\dalphaB$, and
$\dhatm$.
The simulation imagines a sequence of \Sexpr{NP} models, ordered from
least to most preferable.
The first \Sexpr{CP} models are well specified; thereafter the models
are increasingly misspecified.
Each model is assessed with a $Z$-test.
For models $d=1,\dots,$\Sexpr{CP}, the test statistic
$Z_d\sim\mathcal{N}(0,1)$, the standard normal distribution.
For models $d=$\Sexpr{CP+1},$\dots$,\Sexpr{NP}, the test statistic is
distributed as $Z\sim\mathcal{N}\big\{b(d-\Sexpr{CP}),1\big\}$, where
the slope parameter $b$ controls the power of specification tests for
these misspecified models,
which increases with $d$ for values of $d>$\Sexpr{CP}.
Specification p-values are generated by comparing all of these
simulated test statistics against the null distribution
$\mathcal{N}(0,1)$.

<<plotSim1,cache=TRUE,fig.height=4,fig.width=6>>=
ppp%>%
    filter(np==NP,cp==CP,d<=NP,alt%in%c("dmin0.05", "dmin0.25" ))%>%
    mutate(
        #Estimator=c(m="$\\$",alph="$d_{\\alpha}$")[Estimator],
        #alt=c(dmax0.05="$\\dhatU_{0.05}$",dmax0.25="$\\dhatU_{0.25}$",dmin0.05="$\\dhatB_{0.05}$",dmin0.25="$\\dhatB_{0.25}$")[alt],
        slope=paste0("b=",slope)
        )%>%
    ggplot(aes(d,dstar,fill=Estimator))+ #,xintercept=cp))+
    geom_col()+
    geom_vline(aes(xintercept=cp))+
    scale_x_continuous(#name="$d$",
        breaks=1:NP)+
    scale_y_continuous(#name="\\% $\\hat{d}=d$",
        breaks=c(-.75,-.5,-.25,0,.25,.5,.75),labels=c("75","50","25","0","25","50","75"))+
    coord_flip()+
    facet_grid(alt~slope)+
    theme(legend.pos="top")
@

<<plotSim2,cache=TRUE,include=FALSE,fig.height=4,fig.width=6>>=
ppp%>%
    filter(np==NP,cp==CP,d<=NP,alt%in%c("dmin0.05", "dmin0.25" ))%>%
    mutate(
        Estimator=c(m="$\\dhatm$",alph="$d_{\\alpha}$")[Estimator],
        alt=c(dmax0.05="$\\dhatU_{0.05}$",dmax0.25="$\\dhatU_{0.25}$",dmin0.05="$\\dhatB_{0.05}$",dmin0.25="$\\dhatB_{0.25}$")[alt],
        slope=paste0("$b$=",slope)
        )%>%
    ggplot(aes(d,dstar,fill=Estimator))+ #,xintercept=cp))+
    geom_col()+
    geom_vline(aes(xintercept=cp))+
    scale_x_continuous(name="$d$",breaks=1:NP)+
    scale_y_continuous(name="\\% $\\hat{d}=d$",breaks=c(-.75,-.5,-.25,0,.25,.5,.75),labels=c("75","50","25","0","25","50","75"))+
    coord_flip()+
    facet_grid(alt~slope)+
    theme(legend.pos="top")
@



\begin{figure}
<<plotSim,cache=FALSE>>=
ppp%>%
    filter(np==NP,cp==CP,d<=NP)%>%
    mutate(
        Estimator=c(m="$\\dhatm$",alph="$d_{\\alpha}$")[Estimator],
        alt=c(dmax0.05="$\\dhatU_{0.05}$",dmax0.25="$\\dhatU_{0.25}$",dmin0.05="$\\dhatB_{0.05}$",dmin0.25="$\\dhatB_{0.25}$")[alt],
        slope=paste0("$b$=",slope)
        )%>%
    ggplot(aes(d,dstar,fill=Estimator))+ #,xintercept=cp))+
    geom_col()+
    geom_vline(aes(xintercept=cp))+
    scale_x_continuous(name="$d$",breaks=1:NP)+
    scale_y_continuous(name="\\% $\\hat{d}=d$",breaks=c(-.75,-.5,-.25,0,.25,.5,.75),labels=c("75","50","25","0","25","50","75"))+
    coord_flip()+
    facet_grid(alt~slope)+
    theme(legend.pos="top")
@
\caption{Results from \Sexpr{B} simulation runs comparing $\dhatm$ to $\dhatU$
  and $\dhatB$ with $\alpha=0.05$ and $0.25$. Each row compares either
  $\dhatU$ or $\dhatB$ to the same set of $\dhatm$ estimates. Each bar
  represents the percent of runs in which an estimator selects each
  possible model, indexed as $d=1,\dots$,\Sexpr{NP}. Model
  $d=$\Sexpr{CP} (indicated with a horizontal line) is the optimal
  model, with models $d>$\Sexpr{CP} misspecified, and models
  $d<$\Sexpr{CP} well-specified but suboptimal. }
\label{fig:simulation}
\end{figure}

\begin{table}
<<simTable,results='tex'>>=
aaa <- ppp%>%
    filter(cp==CP,np==NP,Estimator=='alph'|alt=='dmax0.05',slope%in%c(0.5,2,3))%>%
    group_by(alt,Estimator,slope)%>%
    summarize(RMSE=sum((d-cp)^2*abs(dstar)),`\\%Opt.`=abs(dstar[d==cp])*100,`\\%$>d^*$`=abs(sum(dstar[d>cp])*100))%>%
    arrange(Estimator)%>%
    mutate(
        alt=c(dmax0.05="$\\dhatU_{0.05}$",dmax0.25="$\\dhatU_{0.25}$",dmin0.05="$\\dhatB_{0.05}$",dmin0.25="$\\dhatB_{0.25}$")[alt],
        Estimator=ifelse(Estimator=='alph',alt,'$\\dhatm$')
    )%>%
    ungroup()%>%
    select(-alt)%>%
    pivot_wider(names_from='slope',values_from=c('RMSE',`\\%Opt.`,`\\%$>d^*$`))

aaa <- aaa[,c('Estimator',outer(c('RMSE',"\\%Opt.","\\%$>d^*$"),c(0.5,2,3),paste,sep='_')%>%as.vector())]
names(aaa) <- c('Estimator',map_chr(strsplit(names(aaa)[-1],'_'),~.[1]))

kbl(aaa,digits=c(0, rep(c(1,0,0),3)),escape=FALSE,booktabs=TRUE)%>%
    add_header_above(c(" "=1,"b=0.5"=3,"b=2"=3,"b=3"=3))
@
\caption{Some results from \Sexpr{B} simulation runs comparing $\dhatm$ to $\dhatU$
  and $\dhatB$ with $\alpha=0.05$ and $0.25$. For $b=0.5,2,3$, the
  root-mean-squared error (RMSE) of each estimator
  $\overline{(\hat{d}-d^*)^2}^{0.5}$ and the percentages each
  estimator chose the optimal model (\%Opt.) or chose a misspecified
  model (\%$>d^*$)}
\label{tab:simulation}
\end{table}

Figure \ref{fig:simulation} and Table \ref{tab:simulation} give the results of the simulation study,
comparing $\dhatm$ to $\dhatB_{0.05}$, $\dhatB_{0.25}$, $\dhatU_{0.05}$
and $\dhatU_{0.25}$, respectively.
Table \ref{tab:simulation} compares all five model selectors at
$b=0.5$, 2, and 3 on three criterion: root mean-squared-error
($RMSE(x)=\overline{(x-d^*)^2}^{0.5}$), a measure of how close, in
general, the estimator is to the optimal value, the percentage
of runs in which it chose the optimal value $d^*$ (\%Opt.) and the
percentage of runs in which it chose a misspecified model, i.e. chose
$d>d^*$ (\%$>d^*$).

In Figure \ref{fig:simulation}, each bar represents the percentage of the times each model selector chose
model $d$, with $d=1,\dots$,\Sexpr{NP}.
Model $d=$\Sexpr{CP} (indicated with a horizontal line) is the optimal
model, with models $d>$\Sexpr{CP} misspecified, and models
$d<$\Sexpr{CP} well-specified but suboptimal.
Each column of Figure \ref{fig:simulation} corresponds to a different
value for the slope parameter $b\in\{0.5,1,1.5,2,2.5,3\}$.
As $b$ increases, so does the power of the specification test,
allowing the test to reject misspecified models at smaller values of
$d>d^*$.
Each row compares the same set of $\dhatm$ to either $\dhatB_{0.05}$,
$\dhatB_{0.25}$, $\dhatU_{0.05}$, or $\dhatU_{0.25}$.

When $b=0.5$, the power to detect misspecification for models $d>d^*$
is relatively low.
$\dhatm$, $\dhatB_{0.05}$, and both $\dhatU$ model selectors tend to
choose models that are too big.
That said, of those four estimators, $\dhatm$ has the smallest root
mean-squared-error (RMSE) and
$\dhatm$ is most likely of all model selectors to choose the optimal
model $d^*$.
$\dhatB_{0.25}$, which is the least likely to
recommend a misspecified model, tends to recommend $d=1$, the smallest
possible, least-optimal model.

As $b$ increases, the performance of all five model selectors
improves.
Throughout, $\dhatm$ is competitive in all three criteria and,
arguably, balances them the best.
At $b=2$, and $b=3$, the $\dhatU$ estimators have better RMSE and tend to pick
the optimal model slightly more often than $\dhatm$, but are more
likely to pick misspecified models.
$\dhatB_{0.25}$ is the least likely to pick misspecified models, but
the models it does pick tend to be much too small.

In general, $\dhatm$ tends to be more conservative than $\dhatU$ or
$\dhatB$ with $\alpha=0.05$ but much less conservative than
$\dhatB_{0.25}$.
Its performance is most similar to $\dhatU_{0.25}$, while being
slightly more conservative.

The appendix gives a larger version of Table \ref{tab:simulation}
including results from when there are $D=20$ candidate models or
$D=10$, as here, and when the optimal model $d^*=2$, $d^*=5$ (as
here), and, when $D=20$, $d^*=10$.
Broadly speaking, the patterns of performance are similar as $D$ and
$d^*$ vary.


\section{Two Data Examples}\label{sec:examples}



\subsection{Lag Order in Autoregression Models: US Total Unemployment}\label{sec:unemployment}


Figure \ref{fig:example}B shows the natural logarithm of the United States total
unemployment rate from 1890 to 2016.
The data were combined from the ``Nelson \& Plosser extended data
set'' provided in the \texttt{urca} library in \texttt{R}
\citep{urca,Rcite}, which covers years 1890--1988, and a downloadable
dataset from the United States Bureau of Labor Statistics, itself
derived from the Current Population Survey, which covers years
1947--2015 \citep{cps}.
The two datasets agree on the overlapping years.

Assume that the time series follows an ``AR($d$)'' model; that is,
\begin{equation}\label{eq:arp}
unemp_t=\mu + \displaystyle\sum_{i=1}^d \phi_i unemp_{t-i}+\epsilon_t
\end{equation}
where $\mu$ and $\{\phi_i\}_{i=1}^d$ are parameters to be estimated
and $\epsilon_t$ is white noise.
In this model, the unemployment in one year is a function of
unemployment rates in the previous $d$ years, but conditionally
independent of even earlier measurements.

Having settled on model (\ref{eq:arp}), the analyst must choose $d$,
the lag order.
sequential specification tests can be useful here \citep[e.g.][]{practitionersGuide}.
Consider the null hypothesis $H_d: \phi_i=0$ for all $i>d$;
a researcher could test a sequence of such null hypotheses, for a set
of plausible values of $d$, and choose the $d$ based on the results.
Other options for choosing $d$ include optimizing
information criteria
\citep{akaike1969fitting,schwarz1978estimating}.
For instance, choosing the model that minimizes \textsc{aic}, defined as
$2(d+2) -2log(\hat{L}_d)$, where $\hat{L}_d$ is the maximized likelihood
of the $AR(d)$ model, or \textsc{bic}, which is defined as $log(n)(d+2) -2log(\hat{L}_d)$.
%\citet{potscher1991effects} points out that differences in \textsc{aic} or
%\textsc{bic} are essentially likelihood ratio test statistics.
%% Sequential specification tests can assist a modeler to
%% choose the smallest model that is still approximately correct, as
%% opposed to the model that maximizes predictive accuracy as measured
%% by, say, mean squared error.
A large literature surrounds this important question \citep[See,
e.g.][and the citations
therein]{mcquarrie1998regression,liew2004lag}. This section is not
meant as a complete treatment, or even an overview, of lag order
selection, but as an illustration of sequential specification tests in a well-known area.

Figure \ref{fig:tspvalues1} gives the p-values from a sequence
likelihood ratio tests, as described in \citet[][Ch.1]{urca}, which
discussed a similar dataset.
For each candidate lag order $d$, the likelihood ratio test compares
twice the ratio of the log likelihoods of $AR(d+1)$ and $AR(d)$ models
to a $\chi^2_1$ distribution.
If the $AR(d+1)$ model fits much better than the $AR(d)$ model, a lag
order of $d$ may not be sufficient.
The p-values follow a stark pattern: for $d<5$, they are close to
zero, while for $d\ge5$, they appear roughly uniformly distributed.

Table \ref{tab:ts}, and vertical lines in Figure \ref{fig:tspvalues1},
show the lag order choices from $\dalphaU$, $\dalphaB$, $\dhatm$, and
$\dhatmab$, which are based on the p-values, and the lag orders that
minimize \textsc{aic} and \textsc{bic}, based directly on the models' likelihood and numbers
of parameters.
Here, smaller models are preferable to larger models, so $\dstar$ is
the smallest acceptable value for $d$.
%This is the opposite of the regression discontinuity case, which
%attempted to find the largest dataset on which to fit the model.

\Sexpr{stopifnot(length(unique(dhatsTSnm[c('alphaU25','dm','dmab','AIC','BIC')]))==1)}
\Sexpr{stopifnot(dhatsTSnm['alphaU05']<dhatsTSnm['dm'])}
\Sexpr{stopifnot(dhatsTSnm['alphaB25']>dhatsTSnm['dm'])}

The change-point selectors $\dhatm$ and $\dhatmab$ both selected a
lag order of \Sexpr{dhatsTSnm['dm']}, consistent with the casual
observation that p-values for lags less than this value are very
small, while those greater appear approximately uniform.
Incidentally, the two information criteria considered, \textsc{aic} and \textsc{bic},
agreed with this choice, as did $\dhatU_{0.25}$.
In contrast, $\dhatU_{0.05}$ chose a smaller lag order of
\Sexpr{dhatsTSnm['alphaU05']}, because the corresponding p-value of
\Sexpr{round(pvalsTS[dhatsTSnm['alphaU05']],3)} slightly exceeds the threshold of 0.05.
\Sexpr{stopifnot(pvalsTS[min(dhatsTS)]>0.05)}
At the other extreme, the $\dalphaB$ selectors both chose very large
models with
$d=$\Sexpr{paste(dhatsTSnm[c('alphaB05','alphaB25')],collapse=' and ')}, due to the presence of
of small p-values of
\Sexpr{round(pvalsTS[dhatsTSnm['alphaB05']-1],3)} and
\Sexpr{round(pvalsTS[dhatsTSnm['alphaB25']-1],3)} at
$d=$\Sexpr{dhatsTSnm['alphaB05']-1} and \Sexpr{dhatsTSnm['alphaB25']-1}.

This example illustrates how considering the entire distribution of
p-values, as $\dhatm$ does, can lead to better model selection than
considering only the small (as in $\dalphaB$) or large ($\dalphaU$)
values.

\begin{figure}
<<tsResults,fig.width=6,fig.height=2.5>>=
dhatsTS <- c(dhatAll(pvalsTS,backwards=TRUE),AIC=which.min(aics),BIC=which.min(bics))
par(mfrow=c(1,1),xpd=FALSE)

dhatDat <- data.frame(value=multLine(dhatsTS,amount=0.05),Selector=names(dhatsTS))
ggplot( mapping=aes(1:length(pvalsTS),pvalsTS))+geom_point(size=2)+labs(x="$d$",y="p-value")+
    #geom_vline(data=dhatDat,aes(xintercept=value,color=Selector),show.legend=TRUE)
  geom_vline(xintercept=unique(dhatsTS))+
  geom_text(mapping=aes(x=unique(dhatsTS),
                         y=0.5,
                         label=map_chr(unique(dhatsTS),
                                       ~paste(names(dhatsTS)[dhatsTS==.],
                                              collapse='\n'))
  ),
  hjust='left'
  )
@
\caption{P-values from likelihood-ratio tests of model fit,
  comparing models AR($d$) with AR($d+1$) in the annual total US
  unemployment rate (logged) time series.}
\label{fig:tspvalues1}
\end{figure}


<<tsTab,results='tex'>>=
orders <- as.data.frame(as.integer(dhatsTS))
names(orders) <- 'Lag Order'
rownames(orders) <- names(dhatsTS)
orders <- t(orders)
print(xtable(orders,caption='Lag order selections for an $AR(d)$ model of the US unemployment time series.', align=rep('c',ncol(orders)+1),
             label='tab:ts'),sanitize.text.function=function(x) x)
@


\subsection{Sequential specification tests in Regression Discontinuity Bandwidth Selection:
  Estimating the Effect of  Academic Probation on College GPAs}\label{sec:rdd}

At many universities, students who fail to achieve a minimum GPA $c$ are
put on academic probation.
\citet{lso} recognized that academic probation can form a regression discontinuity
design, in which treatment is a function of a ``running
variable'' with a pre-determined cutoff.
Specifically, probation $Z$ is a function of
a ``running variable'' $R$, students' GPAs: students with $R<c$ are
put on probation---$Z=1$---and students with $R>c$ are not, $Z=0$.
That being the case, students with GPAs just below $c$ may be
comparable to students with GPAs just above $c$, so comparing these two
sets of students allows researchers to estimate the effect of probation on
outcomes $Y$ (perhaps after adjusting for $Y$'s relationship with $R$).
The challenge becomes defining ``just above'' and ``just below''---that
is, selecting a ``bandwidth'' $bw^*>0$ such that subjects $i$ with $R_i\in
(c-bw^*,c)$ are suitably comparable to subjects with $R_i\in(c,c+bw^*)$.

A number of authors \citep[e.g.]{lee,cft,mattai}
recommend sequential specification tests, using baseline covariates
$X$, as part of the procedure for choosing $bw$.
At a sequence of candidate bandwidths $0<bw_1<\dots<bw_d<\dots<bw_D$,
they recommend testing the equality of covariate means (again, perhaps
after adjusting for $R$) between subjects with $R_i\in
(c-bw_d,c)$ and those with $R_i\in(c,c+bw_d)$, and choosing a
bandwidth $bw^*=bw_{d^*}$.
These are essentially placebo tests---since the treatment cannot
affect baseline covariates, differences in covariate means between
treated and untreated subjects must be an indicator of incomparability
between the groups, or model misspecification.

In a secondary analysis of the academic probation dataset, \citet{lrd}
chose an RDD bandwidth using a set of seven baseline
covariates: students' high-school GPA (expressed in percentiles), age
at college matriculation, number of attempted credits, gender, native
language (English or other), birth place (North America or other) and
university campus (the university consisted of three campuses).
For each covariate $X_k$ and for each candidate bandwidth $bw_d$, they
let $p_{kd}$ be the p-value corresponding the coefficient on $Z$ from
the regression of $X_k$ on $R$ and $Z$, fit to the subset of students
with $R\in (c-bw_d,c+bw_d)$.
These regression models were linear for continuous covariates and
logistic for binary covariates, with heteroskedasticity-consistent sandwich standard errors \citep{sandwich1,sandwich2,sandwich3}.
Then, the ommibus specification p-value for bandwidth $bw_d$ was
$p_d= min\{1,7p_{1d},\dots,7p_{7d}\}$, the minimum of the
Bonferroni-adjusted p-values $p_{kd}$.

\begin{figure}
<<rdd,fig.width=6,fig.height=2.5,results='hide',cache=TRUE>>=
pvalsRD$bws <- floor(pvalsRD$bws*100)/100

windowsT[windowsT==0] <- NA
ds <- c(pvalsRD$bws[unique(windowsT)],ik$bw[1])
dnames <- c(
    vapply(
     unique(na.omit(windowsT)),
      function(ww)
       paste(names(na.omit(windowsT))[na.omit(windowsT)==ww],collapse='\n'),
      'a'),
    'IK')

ggplot(mapping=aes(x=pvalsRD$bws,y=pvalsRD$ps))+geom_point(size=2)+
    geom_vline(xintercept=ds)+
    geom_text(mapping=aes(x=na.omit(ds),y=c(0.8,0.8,0.8,0.7,0.8,0.7),label=dnames),
               inherit.aes=FALSE,hjust="left")+
    labs(x="Bandwidth",y="Balance p-value")
@
\caption{P-values from for balance in all seven covariates
  from the \citet{lso} analysis, following the method in \citet{lrd}. Vertical lines denote bandwidth choices using
  different criteria.}
\label{fig:rdpvalues1}
\end{figure}

The resulting p-values are plotted in  Figure \ref{fig:rdpvalues1},
with bandwidth selections corresponding to
\Sexpr{paste(dnames[-length(dnames)],collapse=', ')}, and
\Sexpr{dnames[length(dnames)]}.
Also plotted is the more conventional bandwidth recommended by
\citet{IK}, denoted IK,
which is based on non-parametric estimates of the curvature of
the regression function of $Y$ on $R$, rather than covariate placebo
tests.
These bandwidth selections are also listed in Table \ref{tab:RDD}.

<<smallpRD,include=FALSE>>=
dsmall <- which.min(pvalsRD$ps[1:20])
bwsmall <- pvalsRD$bw[dsmall]
psmall <- round(pvalsRD$ps[dsmall],3)
@

For most small bandwidths $bw_d$, $p_d$ is fairly large, and in many
cases equal to 1.
This apparent super-uniform distribution is probably due to the
conservative Bonferroni correction applied to the p-values from
individual covariates.
On the other hand, at the smallest candidate bandwidth
$bw_1=$\Sexpr{pvalsRD$bw[1]}, the p-value is $p_1=$%
\Sexpr{round(pvalsRD$ps[1],3)}, and the p-value at the
\Sexpr{dsmall}th bandwidth, $bw_{\Sexpr{dsmall}}=\Sexpr{bwsmall}$,
another small bandwidth, is
$p_{\Sexpr{dsmall}}=\Sexpr{psmall}$.
After around $bw=0.75$, the p-values begin decreasing, until by
$bw=1.5$, the p-values are all close to zero.

<<rddTab,results='tex'>>=

bw <- pvalsRD$bws[windowsT]
bw[is.na(bw)] <- "N/A"

cis <- vapply(ates,
               function(ate) paste0(round(ate$CI['est'],2),
                                    ' (',round(ate$CI['CI1'],2),',',
                                   round(ate$CI['CI2'],2), ')'),
              'a')
cis[names(windowsT)[is.na(windowsT)]] <- "N/A"

tab <- data.frame(
    #dhat=names(windowsT),
    dhat=windowsT,
    bw=bw,
    ATE=cis[names(windowsT)]
    )


tab <- rbind(tab,
             IK=data.frame(
                 dhat=which(pvalsRD$bws==round(ik$bw[1],2)),
                 bw=round(ik$bw[1],2),
                           ATE=paste0(
                               round(-ik$est[1],2),
                               ' (',round(-ik$ci[1,2],2),',',
                               round(-ik$ci[1,1],2),')')
                           ))
names(tab) <- c('$\\hat{d}$','Bandwidth','Effect (95\\% CI)')


xtab <- xtable(tab,caption="Selected regression discontinuity bandwidths (``$\\hat{d}$'' is the point in the sequence selected, and ``Bandwidth'' is the actual bandwidth) using covariate balance tests, or using the method described in \\cite{IK}, along with their associated estimates for the average treatment effect of academic probation on subsequent GPAs (ATE), with 95\\% confidence intervals in parentheses.",label="tab:RDD",digits=c(0,0,2,0))
align(xtab) <- "rccc"

print(xtab,sanitize.text.function=function(x) x)

@

<<reportingRDD,include=FALSE>>=
bwmab <- tab['$\\dhatmab$','Bandwidth']
bwU05 <- tab['$\\dhatU_{0.05}$','Bandwidth']
ciB05 <- paste(round(ates[['$\\dhatB_{0.05}$']]$CI[1:2],2),collapse=',')
@
Since the p-value at the smallest candidate bandwidth,
$p_1=\Sexpr{round(pvalsRD[['ps']][1],3)}<0.25$, the model selector
$\dhatB_{0.25}$ does not select anything---there is no $d'$ small
enough so that $p_{d}<0.25$ for all $d\le d'$.
Similarly, the very low p-value at the \Sexpr{dsmall}th bandwidth
causes $\dhatB_{0.05}$ to select a relatively small bandwidth of \Sexpr{bwsmall}.
This illustrates the sensitivity of $\dhatB$ to outlier p-values at
small $d$.

The remaining selectors all recommend bandwidths greater than 1,
ranging from
$\dhatmab$, which recommends bandwidth $bw_{\dhatmab}$=\Sexpr{bwmab} to
$\dhatU_{0.05}$, which recommends bandwidth
$bw_{\dhatU_{0.05}}=$\Sexpr{bwU05}.
As in the simulation and the unemployment example, $\dhatm$ and
$\dhatU_{0.25}$ are quite close to each other.
The similarity of the IK bandwidth of $\Sexpr{tab['IK','Bandwidth']}$
to the bandwidths selected by $\dhatU$ and $\dhatm$ suggests an
encouraging agreement, in this example, between covariate-based
bandwidth selection and the more conventional RDD approach.

It is worth noting that super-uniformity of the p-values for small
bandwidths inflates the sums $\sum_{t\le d} (p_t-1/4)$ in
\eqref{eq:dhatm}.
Therefore, in this case $\dhatmab$, which relies less on the uniform
model for p-values under $H_0$, may be a more appropriate choice
than $\dhatm$.

Table \ref{tab:RDD} also lists estimated treatment effects of academic
probation on students' subsequent GPAs, along with 95\% confidence
intervals.
The effects were estimated following the method described in
\citet{lrd}, with the exception of the estimate for the IK bandwidth
which used local linear regression, as implemented in the \texttt{R}
package \texttt{rdd} \citep{rdd}.
With the exception of the effect corresponding to $\dhatB_{0.05}$, all
estimated effects are roughly equal, about
\Sexpr{tab['IK','Effect']} grade points.
Actually, the confidence interval corresponding to the $\dhatB_{0.05}$
bandwidth,
(\Sexpr{ciB05})
is wide enough to contain both a negative academic probation effect
along with all of the other estimated effects and confidence intervals.
The conservativism of $\dhatB_{0.05}$ prevents efficient effect
estimation; the conservativism of $\dhatB_{0.25}$ prevents
estimation altogether.




\section{Discussion}\label{sec:discussion}


As long as data analysts use specification tests and p-values to
select their models, decision rules translating a sequence of p-values to a
model choice will be necessary.
Currently, the most common approach compares the p-values to a
pre-specified threshhold.
This approach turns the logic of null
hypothesis testing on its head, using p-values to identify
well-specified models---i.e. true null hypotheses--rather than to
reject misspecified models.
Moreover, the $\dhatB_{\alpha}$ approach, by failing to control the
familywise type-I error rate, can be extremely conservative,
for instance recommending very high lag order in the unemployment
example of Section \ref{sec:unemployment} and very small bandwidths
(or none at all) in the academic probation example of Section \ref{sec:rdd}.
In contrast, $\dhatU_{\alpha}$ does control familywise type-I error rates.
However, both threshhold-based approaches, $\dhatB$ and $\dhatU$,
require specifying a threshhold, and there is rarely any clear
guidence on how to do so.

The alternatives introduced here, $\dhatm$ and $\dhatmab$, drawn from
the change-point literature, skirt these issues entirely.
Rather than using p-values to reject (or accept) null hypotheses, they
examine the full distribution of p-values.
They require no arbitrary threshhold to be specified.
As shown in the simulation study and the two data examples, they tend
to avoid the conservativism and outlier sensitivity of $\dhatB_{0.25}$
and the anti-conservativism of $\dhatU_{0.05}$ (indeed, Proposition
\ref{prop:conservative} states that $\dhatm$ is asymptotically conservative).

Actually, the simulation results and examples show that $\dhatm$ tends
to agree with $\dhatU_{0.25}$, which itself performs rather well.
This suggests that $\dhatU$ with a default threshhold of 0.25 may be a
good option for data analysts who wish to continue using
threshhold-based approaches.

There are several open questions regarding $\dhatm$'s behavior and
use.
First, it is unclear whether or when the more flexible version
$\dhatmab$ should be preferred to $\dhatm$; there is good reason to
expect it to perform better when sample sizes are small, but is there
a cost associated with using $\dhatmab$ in larger samples?
Further, there may be ways to construct sequential specification tests
in a way that improves $\dhatm$'s performance.
How to best construct specification tests for different model
selectors is a topic for future research.

Ultimately, the goal of model selection is to produce parameter
estimates or predictions with desired properties.
Ideally, researchers would select a model with this end in mind;
however, the effect of model selection on final estimates or
predictions depends heavily on specific circumstances.
That said, a careful study of the effect of $\dhatm$ on estimates and
predictions in a wide range of cases could be useful.

Model selectiors $\dhatm$ and $\dhatmab$ may be particularly useful in
measurement modeling, where model choice based on sequences of
p-values are common (such as to select the number of components in factor or
latent class analysis).
The encouraging performance of $\dhatm$ when $d^*=2$ suggests that
$\dhatm$ may be appropriate even when the true number of components is
small.

\bibliographystyle{plainnat}
\bibliography{sst}

\appendix
\section{Simulation Results with different $D$ and $d^*$}

Table \ref{tab:fullSimulation} gives the results of Table
\ref{tab:simulation} for varying values of $D$ (the total number of
candidate models) and $d^*$ (the optimal model).

\begin{table}[!h]
<<simTable2010,results='tex'>>=

aaa <- ppp%>%
    filter(Estimator=='alph'|alt=='dmax0.05',slope%in%c(0.5,2,3))%>%
    group_by(cp,np,alt,Estimator,slope)%>%
    summarize(RMSE=sum((d-cp)^2*abs(dstar)),`\\%Opt.`=abs(dstar[d==cp])*100,`\\%$>d^*$`=abs(sum(dstar[d>cp])*100))%>%
    arrange(np,cp,Estimator)%>%
    mutate(
        alt=c(dmax0.05="$\\dhatU_{0.05}$",dmax0.25="$\\dhatU_{0.25}$",dmin0.05="$\\dhatB_{0.05}$",dmin0.25="$\\dhatB_{0.25}$")[alt],
        Estimator=ifelse(Estimator=='alph',alt,'$\\dhatm$')
    )%>%
    ungroup()%>%
    select(-alt)%>%
    pivot_wider(id_cols=c(cp,np,Estimator),names_from='slope',values_from=c('RMSE',`\\%Opt.`,`\\%$>d^*$`))

aaa <- aaa[,c('np','cp','Estimator',outer(c('RMSE',"\\%Opt.","\\%$>d^*$"),c(0.5,2,3),paste,sep='_')%>%as.vector())]
names(aaa) <- c('$D$','$d^*$','Est.',map_chr(strsplit(names(aaa)[-c(1:3)],'_'),~.[1]))

kbl(aaa,digits=c(0, rep(c(1,0,0),3)),escape=FALSE,booktabs=TRUE)%>%
    row_spec(0, angle = 45)%>%
    add_header_above(c(" "=3,"b=0.5"=3,"b=2"=3,"b=3"=3))%>%
    collapse_rows(columns = 1:2,valign="middle")
@
\caption{Simulation runs with the total number of models to compare
  $D\in\{10,20\}$ and the optimal model $d^*\in\{2,5,10\}$ comparing $\dhatm$ to $\dhatU$
  and $\dhatB$ with $\alpha=0.05$ and $0.25$. For $b=0.5,2,3$, the
  root-mean-squared error (RMSE) of each estimator
  $\overline{(\hat{d}-d^*)^2}^{0.5}$ and the percentages each
  estimator chose the optimal model (\%Opt.) or chose a misspecified
  model (\%$>d^*$)}
\label{tab:fullSimulation}
\end{table}



\end{document}
