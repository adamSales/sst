\documentclass[12pt]{article}

%\usepackage{endfloat}
% \usepackage{type1ec}
% \usepackage{subcaption}
% \usepackage{fullpage}
% \usepackage{enumerate}
% \usepackage{graphicx}
% \usepackage{graphics}
% \usepackage{multirow}
\usepackage{comment}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{setspace}
%\usepackage{verbatim}
\usepackage{natbib}
\usepackage{bm}
\usepackage{pdflscape}
%\usepackage{tikz}
%\usepackage{xr}
\usepackage[colorlinks=true,linkcolor=green]{hyperref}

\newcommand{\dalphaU}{\bar{\hat{d}}_\alpha}
\newcommand{\dalphaB}{\underline{\hat{d}}_\alpha}
\newcommand{\dstar}{d^*}
\newcommand{\ps}{\bm{p}_D}
\newcommand{\dhat}{\hat{d}}
\newcommand{\dhatU}{\bar{\hat{d}}}
\newcommand{\dhatB}{\underline{\hat{d}}}
\newcommand{\dhatm}{\hat{d}_M}
\newcommand{\dhatmab}{\hat{d}^{ab}_M}
\newcommand{\hedged}{H_{0d}^{edge}}
\newcommand{\htotd}{H_{0d}^{tot}}
\newcommand{\EE}{\mathbb{E}}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}


\doublespacing

\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newtheorem{conjecture}{Conjecture}
\newtheorem{ce}{Counter-Example}
%\newtheorem{ass}{Assumption}
\newtheorem{alg}{Algorithm}
%\newtheorem*{ass*}{Assumption}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}

\newenvironment{ass}[2][Assumption:]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2}.]}{\end{trivlist}}

<<include=FALSE>>=
require(ggplot2)
library(knitr)
library(tikzDevice)
library(xtable)
opts_chunk$set(
echo=FALSE, results='asis',cache=FALSE,warning=FALSE,error=FALSE,dev='tikz',message=FALSE,autodep = TRUE
    )

options(tikzDefaultEngine = "pdftex")



@

<<functions>>=

dhatm <- function(ps){
    M <- cumsum(ps-1/4)
    which.max(M)
}

dhatAlpha <- function(ps,alpha){
    D <- length(ps)
    max(seq(D)[ps>=alpha],na.rm=TRUE)
}

ds <- function(ps,alphas=c(0.05,0.15,0.25)){
    dhat <- list()
    for(a in alphas){
        dhat[[paste0('alpha',a)]] <- dhatAlpha(ps,a)
    }
    dhat[['Mallik']] <- dhatm(ps)
    dhat
}


multLine <- function(dhats,amount=0.1){
    ttt <- table(dhats)
    if(max(ttt)==1) return(dhats)
    for(d in unique(dhats)){
        num <- ttt[paste(d)]
        if(num>1)
            dhats[dhats==d] <- dhats[dhats==d]+seq(-floor(num/2)*amount,by=amount,length=num)
    }
    dhats
}

cols <- colors()[c(red=552,lightblue=636,mag=642,green=255,orange=499,black=153,purple=469,darkblue=477)]
cbbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")


smoothplot <- function(x,y,...){


    naFlag <- is.na(x) | is.na(y)
    x <- x[!naFlag]
    y <- y[!naFlag]


    tmp <- vapply(unique(x), function(xval) c(
        av=mean(y[x==xval],na.rm=TRUE),
        sizes = length(y[x==xval])
        ),numeric(2))

    tmp['sizes',] <- 2*tmp['sizes',]/max(tmp['sizes',])
    plot(unique(x),tmp['av',],cex=tmp['sizes',],...)
    abline(v=0,lty='dotted')


}


facet2 <- function(st,curv){
    ns <- sort(unique(vapply(strsplit(names(st),'_'),function(x) as.numeric(x[2]),1)))
    dat <- NULL
    cname <- ifelse(curv,'curved','mono')
    for(n in ns){

        run <- do.call('rbind',lapply(st[[paste0(cname,'_',n)]],function(x) x$full))
        st1 <- do.call('c',as.data.frame(run))
        st1 <- data.frame(est=st1,method=rep(colnames(run),each=nrow(run)))
        st1$selector <- ifelse(grepl('max',st1$method),'$\\bar{d}_\\alpha$',
                               ifelse(grepl('min',st1$method),'$\\underline{d}_\\alpha$','$\\hat{d}_M$'))
        st1$method <- gsub('\\\\\\hat\\{d\\}\\^\\{m[axin]*\\}_\\{','$\\\\\\alpha=',st1$method)
        st1$method <- gsub('\\}','$',st1$method)
        st1$method <- gsub('\\\\\\hat\\{d\\$\\^\\{flex\\$_M','a,b',st1$method)
        st1$method <- gsub('\\\\\\hat\\{d\\$_M','0.5,1',st1$method)

        st1$method <- factor(st1$method)
        st1$n <- n

        dat <- rbind(dat,st1)
    }

    dat$n <- factor(dat$n)
    levels(dat$n)=paste0('n=',levels(dat$n))

    p <- ggplot(dat, aes(x=selector, y=est,fill=method)) +
        geom_violin(bw=1,position=position_dodge(.5))+
        geom_boxplot(width=0.1,position=position_dodge(.5))+
        geom_hline(yintercept=10,lty=2)+
            labs(title=paste('Imbalance:',ifelse(curv,'Sinusoidal','Linear')),x='$\\hat{d}$')+
                facet_wrap(~ n,ncol=3)
    p
}


@

<<Winselectfunctions>>=
source('src/winSelect.r')

@

<<lsoAnalysis,cache=TRUE>>=
require(RItools)
require(foreign)
source('src/winSelect.r')
ap <- read.dta('data/AEJApp2008-0202_data/data_for_analysis.dta')
#### downloaded 1/2/17 from https://www.aeaweb.org/aej/app/data/2008-0202_data.zip

ap$r <- round(ap$dist_from_cut,2)
ap <- ap[abs(ap$r)<=0.5,]
ap$r <- round(ap$r+0.005,3)

ap$z <- ap$r<0

bws <- sort(unique(abs(ap$r)))

balform <- z~hsgrade_pct+age_at_entry+totcredits_year1+male+loc_campus1+loc_campus2+english+
    bpl_north_america

balTestT <- function(bw){
    xBalance(balform,data=ap[abs(ap$r)<=bw,], report='chisquare.test')$overall$p.value[1]
}

psT <- vapply(bws,balTestT,1)
windowsT <- dhatAll(psT,alphas=c(0.05,0.15))

balTestT1 <- function(bw){
    datbw <- ap[abs(ap$r)<=bw,]
    with(datbw,wilcox.test(hsgrade_pct[z],hsgrade_pct[!z]))$p.value
}

psT1 <- vapply(bws,balTestT1,1)
windowsT1 <- dhatAll(psT1,alphas=c(0.05,0.15))

ate <- function(bw){
    mod <- lm(nextGPA~z,subset=abs(r)<=bw+0.005,data=ap)
    summary(mod)$coef['zTRUE',1:2]
}

ates <- vapply(windowsT,function(w) ifelse(rep(w>0,2),ate(bws[w]),rep(NA,2)),c(0.1,0.2))
ates2 <- vapply(windowsT1,function(w) ifelse(rep(w>0,2),ate(bws[w]),rep(NA,2)),c(0.1,0.2))



@
<<timeSeriesAnalysis,cache=TRUE>>=

library(urca)
data(npext)
#npext <- read.csv('data/npext.csv')
y <- npext$unemploy[31:nrow(npext)]
y2 <- data.frame(Year=1890:1988,unemp=y)

new <- read.csv('data/unemployment2.csv',header=F)
### new was downloaded 1/25/17 from BLS/CPS: https://www.bls.gov/cps/cpsaat01.xlsx


new <- with(new,data.frame(year=V1,unemp=log(V16)))
y <- c(y,new$unemp[new$year>1988])
y <- ts(y,start=1890)

lrTest <- function(p,edge=TRUE){
    stopifnot(p<20)
    p <- p+1
    big <- ifelse(edge,p+1,21)
    llBig <- logLik(arp[[big]])
    llSmall <- logLik(arp[[p]])
    lrtest <- as.numeric(2*(llBig-llSmall))
    pchisq(lrtest , df = 1, lower.tail = FALSE)
}


arp <- lapply(0:20,function(p) arima(y,order=c(p,0,0)))
pvalsTS <- vapply(0:19,lrTest,1)
#pvalsFull <- vapply(0:19,lrTest,1,edge=FALSE)

aics <- vapply(arp,AIC,1)
bics <- vapply(arp,BIC,1)

npTS <- length(pvalsTS)
dhatsTS <- c(dhatAll(pvalsTS,backwards=TRUE,alphas=c(0.05,0.15)),AIC=which.min(aics),BIC=which.min(bics))

dhatsTSnm <- setNames(dhatsTS,c('alphaU05','alphaU15','alphaB05','alphaB15','dm','dmab','AIC','BIC'))
#source('src/timeSeries2.r')
@
\title{Sequential Specification Tests to Choose a Model: A
  Change-Point Approach}

\author{Adam Sales}

\begin{document}

\maketitle

\section{Introduction}

One of many mis-matches between best practices recommended by
statisticians and practice in quantitative research regards model
selection.
Statisticians conceptualize model selection as a tradeoff between bias
and variance.
Many quantitative researchers think about model selection as choosing
the best model that satisfies the assumptions of their intended
statistical test or estimator---essentially minimizing variance while
constraining bias at zero.
This latter outlook leads researchers towards hypothesis tests of
model assumptions; in particular, a sequence of hypothesis tests, for
a sequence of models, ordered by preferability.
The best model whose assumptions ``pass'' a hypothesis test is chosen.

Do hypothesis tests make any sense in model selection?
For one, ``all models are wrong''
\citep[p. 2]{modelsWrong} and ``there's no such thing as unbiased
estimation'' \citep{gelmanBlogUnbiased}, so the search for a correct
model might be hopeless, and therefore pointless.
Further, the logic of null-hypothesis testing seems incompatible
with this framework.
The results of a null hypothesis test, of course, are never evidence
in favor of a null hypothesis---null hypotheses can only be rejected,
not accepted.

On the other hand, ``some models are useful,'' and depending on their
intended use, their usefulness may depend on \emph{approximate}
correctness.
If so, hypothesis tests may have a role to play.
Specification tests already exist for most common models, and
they are regularly taught in introductory quantitative methods
classes.
If their use in model selection could be made conceptually sound, they
are likely to be actually used---and maybe even correctly.

This paper will borrow a clever idea from change-point or threshhold
estimation to the more general problem of model selection from
hypothesis tests---more accurately, p-values.
\citet*{mallik} points out that in a process with a change point,
the p-values from a sequence of tests of a null regression function
are uniformly-distributed as long as the regression function is
correct, but assymptotically zero when the function is not correct.
They use this dichotimous behavior to construct a simple, consistant
estimator of the change-point---the point at which the null model
stops being correct.

In the same way, their estimator can choose the change-point in
a sequence of models, when models stop being correct.
In doing so, it shifts the model selection rationale away
from the logic of hypothesis testing and towards the logic of
estimation.
In the tradition of constructing confidence intervals from hypotheis
tests and \citet{hodgesLehmann}, their estimator exploits
the behavior of hypothesis tests to estimate quantities of interest.
Further, as opposed to model selectors based on strict
hypothesis-testing logic, an individual test result will itself not
drive the change-point estimator, which is instead based on the entire
sequence of p-values.
Thus, the change-point view of model selection is arguably conceptually
more satisfying and practically more reliable than the conventional
test-based approach.

The following sub-section will briefly introduce two running examples
of sequential specification tests: choosing a bandwith for a
regression discontinuity design and choosing a lag order for a
time-series model.
Next, section \ref{sec:setup} will review the formalism of SSTs and
discuss common SST-based model selectors.
Section \ref{sec:change-point} will introduce the new method, section
\ref{sec:simulation} will demonstrate some of its properties in a
simulation study, \ref{sec:examples} will apply it to the running
examples, and \ref{sec:discussion} will conclude.

\subsection{SSTs in Regression Discontinuity and Time Series}\label{sec:exampleIntro}

\begin{figure}
<<examplePlots,fig.width=6,fig.height=4,dev='pdf'>>=

par(mfrow=c(1,2))
smoothplot(ap$dist_from_cut,ap$nextGPA,xlab='First-Year GPA (Distance from Cutoff)',ylab='Avg Subsequent GPA',sub='(A)')
modTrt <- lm(nextGPA~dist_from_cut,data=ap[ap$dist_from_cut<0,])
modCtrl <- lm(nextGPA~dist_from_cut,data=ap[ap$dist_from_cut>=0,])
lines(x=c(0,min(ap$dist_from_cut)),y=c(coef(modTrt)[1],coef(modTrt)[1]+coef(modTrt)[2]*min(ap$dist_from_cut)),
          xlim=c(min(ap$dist_from_cut),0),col='red')
lines(x=c(0,max(ap$dist_from_cut)),y=c(coef(modCtrl)[1],coef(modCtrl)[1]+coef(modCtrl)[2]*max(ap$dist_from_cut)),
          xlim=c(0,max(ap$dist_from_cut)),col='blue')
abline(v=0,lty='dotted')

plot(y,xlab='Year',ylab='US Total Unemployment',sub='(B)')

@
\caption{Two data examples for SSTs. Plot (A) shows data from
  \citet{lso}---subsequent grade point averages (GPAs) for students
  at a large Canadian university, as a function of first-year
  GPAs. Subsequent GPAs are averaged by first-year GPAs, which are
  centered at the academic probation cutoff (dotted line), and the sizes
  of the plotted points are proportional to the number of students
  with each first-year GPA. The red and blue lines are linear least-squares
  fits on either side of the cutoff. Students with first-year GPAs
  to the left of  the cutoff are put on probation. Plot (B) shows a
  time-series of log annual United States total unemployment from 1890 to
  2015. Data were combined from \citet{urca} and \citet{cps}.}
\label{fig:examples}
\end{figure}
%%% NOTE: schotman & van dijk used AR(4) for unemployment


Figure \ref{fig:example} displays two datasets that will serve as
illustrations of SSTs.
Section \ref{sec:examples} will discuss both of these examples in more
detail.
The brief overview here will be helpful to fix ideas.

Figure \ref{fig:example}A plots data that \citet{lso} used to estimate the effect of academic
probation.
Students at an unnamed large Canadian University were put on academic
probation---simultaneously given extra help and threatened with
suspension---if their first-year cumulative grade point averages
(GPAs) fell below a cutoff.
This is an example of a regression discontinuity design (RDD) \citep{thistlewhiteCampbell}, in
which treatment (in this case academic probation) is assigned if a
numeric ``running variable'' $R$ (first-year GPA) falls below (or above) a
pre-specified cutoff $c$.
Typically \citep[e.g.][]{imbensLemiuxRDD,angristLavy,lee} analysts
will fit regression models $Y=f_1(R)+\epsilon$ and $Y=f_2(R)+\epsilon$
to data on either side of the cutoff, modeling the relationship
between $R$ and an outcome of interest $Y$.
The difference between the models' predictions when $R$ is set equal
to the $c$ is interpreted as a ``local
average treatment effect,'' roughly speaking the treatment effect when
the running variable is equal to the cutoff \citep{HTV}.
Figure \ref{fig:exampe}A shows one of the outcomes \citet{lso} considered,
students' subsequent GPAs, along with linear regression models below
(in red) and above (blue) $c$, which is signified with a dotted
line.
A simpler alternative approach, suggested in \citet{cft}, models the
relationships between $Y$ and $R$ on either side of $c$ as constant,
and treats the data as if they were generated by a randomized
experiment.

Of course, misspecified regression models will lead to biased
treatment effect estimators.
To minimize the influence of model misspecification, researchers will
typically fit the regression models using only subjects for whom $R\in
\mathcal{W}_b\equiv (c-b,c+b)$ for some bandwidth $b>0$.
A number of options exist for choosing the RDD bandwidth, including
cross-validation \citep{ludwigMiller} and assymptotic minimization of
mean-squared-error \citep{IK}.
\citet{cft} and others \citet{salesHansen,angristWanna} suggest SSTs
of covariate balance---at a sequence of candidate bandwidths $b$, test
for the presence of a ``treatment effect'' on a pre-treatment
covariate $X$, referred to as covariate imbalance.
A window choice $\mathcal{W}_b$ that, paradoxically, leads to a
statistically significant
non-zero treatment effect on a covariate is unacceptible; on the other
hand, larger windows include larger datasamples, yielding higher
precision.
Therefore, SSTs could be used to choose the largest $b$ for which a
hypothesis test fails to reject the hypothesis of covariate balance.

Figure \ref{fig:example}B shows the annual total unemployment rate in the
United States from 1890 to 2015.
One of the simpler models for time series such as these is an order
$p$ autoregression, or $AR(p)$ under which the value of the time
series at point $t$ may depend on its historical values at
$t-1,...,t-p$ but, conditional on those, is independent of values at
points before $t-p$.
SSTs can be useful here, too: researchers may test model fit for a
sequence of lag orders $p$, and choose the smallest $p$ that the tests
fail to reject.
Here a smaller lag orders $p$ are preferable since they lead to more
parsimonious models and more precise estimates.



\section{The Setup, in General}\label{sec:setup}

Say, in specifying a model, a research must choose from a discrete,
ordered, set of specifications $d=1,2,\dots,D$.
The resulting model must satistfy testible assumption $\mathcal{A}$.
Assume that either $\mathcal{A}$ is false for all $d$, or that for
some $1\le \dstar\le D$, $\mathcal{A}$ is true for $d\le \dstar$ and false
for all $d>\dstar$.
Further assume that if $\dstar$ exists, it is the optimal choice---for
instance, the smallest model, or the biggest dataset, that satisfies
$\mathcal{A}$.
Finally, assume the researcher has chosen a valid, unbiased test of
$\mathcal{A}$ and caluculated p-values for each $d$:
$\bm{p}_D=p_1,\dots,p_d,\dots,p_D$.
The procedure here is to use $\bm{p}_D$ to choose a specification
$\hat{d}$ that is as large as possible without violating
$\mathcal{A}$.


A common choice for $d$ in this scenario relies on the logic of null
hypothesis testing: for a pre-specified $\alpha \in (0,1)$, let
\begin{equation*}
\dalphaU \equiv max\{d : p_d>\alpha\}.
\end{equation*}
That is, $\dalphaU$ is the largest value of $d$ for which the
null hypothesis that $\mathcal{A}$ is true for $d\le \dalphaU$
cannot be rejected at level $\alpha$.
Although it may seem as though the multiplicity of tests invovled in
this procedure invalidates the null hypothesis framework, it turns out
that this is not the case: the ``stepwise intersection-union
pricipal'' \citet{berger1988, rosenbaum2008,hansen2015} insures that the
family-wise error rate is maintained.
That is, the probability of falsely rejecting the null---choosing
$\dalphaU<\dstar$, is bounded by $\alpha$.
$\dalphaU$ is the specification that would result from testing null
hypotheses backwards: for $d'=D,D-1,\dots,d,\dots,1$, test $H_{0d'}:$
$\mathcal{A}$ is true for $d\le d'$.
Then, stop testing at $d'=\dalphaU -1$---the first $d'$ for which
$p_{d'} \ge \alpha$; reject all null hypotheses $H_{0d'}$ for which
$d'\ge \dalphaU$, and fail to reject the rest.
This protects the family-wise error rate of $\alpha$ since rejecting
\emph{any} true null implies rejecting the first true null---a
probability $\alpha$ event.

Another common choice for $\hat{d}$  \citep[e.g.][]{lutkepohl2005new}, say $\dalphaB$,
does not have this property.
Let
\begin{equation}
\dalphaB\equiv min\{d: p_d<\alpha\}-1
\end{equation}
$\dalphaB$ selects $\hat{d}$ to be the largest value of $d$ before the first
significant p-value.
This is equavalent to the opposite procedure as $\dalphaU$: start with the $d'=1$
and test sequentially for larger values of $d'$ until the first
rejection, at $\dalphaB$, then stop; reject all null
hypotheses $H_{0d'}$ for $d'\ge \dalphaB$ and fail to
reject the rest.
This procedure does not control family-wise error rates---it is likely
to reject more than $100\alpha$\% valid specifications.

This paper will focus on two data scenarios for SSTs, corresponding to
the two examples in Section \ref{sec:exampleIntro}.
In the first, the SSTs help determine which data are included in the
analysis.
For instance, choosing the bandwidth of a regression discontinuity
design, or choosing the parameters of a matching design.
In this scenario, each choice $d$ corresponds to rows in the dataset
that could be included in the analysis.
Formally, let $\mathcal{I}=\{1,\dots,N\}$, indices for $N$ candidate
cases to be fit in a model.
Then let $\mathcal{I}=\mathcal{i}_1\cup\mathcal{i}_2\cup\dots\cup
\mathcal{i}_d \cup \dots union \mathcal{i}_D$.
The choice of $\hat{d}$ means fitting the model to the dataset
including each of these subsets,
$\mathcal{I}_d=\mathcal{i}_1\cup\dots\mathcal{i}_d$.
Note that the sets denoted with lower-case $\mathcal{i}_d$ are disjoint,
$\mathcal{i}_d \cap \mathcal{i}_{d'}=\emptyset$,
those denoted with upper-case $\mathcal{I}_d$ are nested---$d>d'$
implies $\mathcal{I}_{d'}\subset \mathcal{I}_d$, and the full set of
indices, noted without a subscript, $\mathcal{I}=\mathcal{I}_D$.
Finally, let $n_d=|\mathcal{i}_d|$, where $|\cdot |$ denotes
cardinality, and $\bar{n_d}=|\mathcal{I}_d |$.

A second scenario applies when the dataset is fixed, but the model is
not.
Here, $d$ indexes a \emph{pre-specified} sequence of models.
For instance, using SSTs to choose the lag order $p$ in an $AR(p)$
time series model.
Then let $\bm{X}$ denote the full set of variables, $\bm{x}_d$ denote
the set of variables that would be \emph{subtracted} in the $d^{th}$ step
of the sequence, and $\bm{X}_d=\bm{x}_{d+1} \cup \dots \cup \bm{x}_D$
denote the set of variables that would be included in the analysis,
were the analyst to choose $d$.
Note here that bigger values of $d$ correspond to smaller models.
In this scenario, the sample size is fixed at $N$.

\subsection{Model Selection and the Logic of Null Hypothesis Testing}
In order to avoid certain methodogical mistakes, it may be helpful to
clarify some of the conceptual distinctions between SSTs and
conventional null hypothesis tests (NHTs).
The logic of NHTs is familiar to anyone who has taken (and understood)
even the most basic college statistics course; nonetheless we restate
it here to distinguish it from the logic of SSTs.
Typically, researchers use NHTs to reject a null hypothesis that they
consider uninteresting---most of the time, that a model parameter is
equal to zero---and interpret rejection as evidence in favor of an
interesting alternative hypothesis.
NHTs cap the probability of a type-I error---falsely rejecting a true
null hypothesis---and, given that constraint, seek to minimize the
probability of a type-II error, failing to reject a false null
hypothesis.

SSTs reverse some of these elements; most importantly, the goal of
SSTs is to identify specifications in which an assumption
$\mathcal{A}$ is plausible, rather than to identify true alternative
hypothesis.
In the same vein, type-II errors are typically of more concern for
SSTs than for typical NHTs, and type-I errors are less problematic.
In fact, a type-II error from a specification test could lead a
researcher to fit a misspecified model, which in turn may inflate the
probability of a type-I error in her final outcome analysis.
For that reason, some methodologists recommend setting $\alpha$
substantially higher for specification tests than for NHTs in outcome
analyses.
Still, the hypothesis testing framework, in the case of pont null
hypotheses, does not allow a researcher to
fix the type-II error rate at a pre-specified value, and then optimize
the type-I error rate, though that might be ideal for
specificaton tests.

In fact, in continuous data models with continuous parameter spaces,
no hypothesis test can provide any evidence in favor of a point null
hypothesis.
For instance, take the common $H_0: \theta=0$, for some parameter
$\theta\in \mathbb{R}$.
In finite samples, for any type-I or type-II error rate, there will
always be some plausible alternative hypothesis $H_a
\theta=\epsilon\ne 0$.
Further, in these situations, finite sample estimates $\hat{\theta}$
will almost surely be non-zero.
This is important to state to avoid misinterpretations of SST
procedures as providing evidence, or showing, that an assumption
$\mathcal{A}$ is true for certain specificatons $d$.
A common Bayesian argument (e.g. \citealp[][p. 439]{kadanePrinciples};
\citealp{gelmanBlog}) states that, theoretically, nearly all null
hypotheses are false anyway---so testing them makes little sense.
In the case of specification tests, that means that an assumption
$\mathcal{A}$ can be assumed to be false for all $d$ without even
conducting a test; in other words, ``all models are wrong''
\citep[p. 2]{modelsWrong}.

``But some are useful.''
In practice there is much to be gained by considering assumptions such
as $\mathcal{A}$.
In this framework, it may indeed make sense to identify a set of
specifications $d$ for which $\mathcal{A}$ is plausible, or
approximately true, and
SSTs can be useful in this regard---as long as they are understood
correctly, and not as providing evidence \emph{for} $\mathcal{A}$.

In many scenarios the choice of $d$ involves a bias-variance tradeoff:
if $d>\dstar$, then $\mathcal{A}$ is false and the resulting analysis
will be biased.
On the other hand, a sub-optimal choice for $d$ often means a
high-variance estimate.
For instance, in the RDD bandwidth case, choosing $d>\dstar$ might mean
fitting a misspecified model to $Y$ and $R$, but choosing $d<<\dstar$
means discarding data that can boost precision.
Rather than choosing a criterion, such as mean-squared-error, that
balances bias and variance, the SST approach may be seen as an attempt to hold
bias at approximately zero, and minimize variance under that
constraint.
Granted, this is obviously an overly-optimistic take on model fitting;
still, SSTs hope to contrain bias to be approximately zero, and from there
minimize variance.


\subsection{More Reservations with Null Hypothesis Testing for Model
  Selection}

Applying a strict hypothesis-testing framework to SSTs for model
selection has some additional drawbacks.
First, it requires researchers to choose a test-level $\alpha$. While
using tuning parameters to mediate the bias-variance tradeoff is not
uncommon in statistics, the level $\alpha$ is a particularly hard
parameter to choose.

\citet{granger} poses an additional problems with the use of
hypothesis tests to choose a model: the need to specify a null
hypothesis. In their words (p. 179),
\begin{quote}
Whenever a hypothesis test is used to choose between two models, one
model must be selected as a null hypothesis. In most instances, this
is usually the more parsimonious model and typically a nested test is
applied. Often it is difficult to distinguish between the two models
because of data quality (multicol- linearity, near-identification, or
the models being very similar such as in testing for integration). In
such cases, the model chosen to be the null hypothesis is unfairly
favored.
\end{quote}
In other words, because of the structure of null hypothesis tests,
which constrain the type-I error rate, the null model is unfairly
favored.
In our terminology, $\hat{d}$ is likely to be too small, perhaps $\EE
\hat{d}<\dstar$.
However, such a bias (if it indeed exists) needn't doom SSTs---an
underestimated $\hat{d}$ is merely sub-optimal. In our setup, choosing
$\hat{d}$ to be too low will yield and inefficient, but still valid,
model.
Would that every statistical model were valid yet suboptimal!

More broadly, perhaps, one might argue that null hypothesis tests are
design to rule out hypotheses that are inconsistant with the data, not
to estimate parameters.
However, as \citet{hodgeslehmann} showed, these aims are not
contradictory---tests that rule out implausible hypothesis may also
point researchers towards the correct answer.

Moving from rejecting implausible specifications to estimating optimal
specifications requires a theory, or at least a reasonable heuristic.
The following section will suggest one.

\section{Finding the Change-Point}\label{sec:change-point}
In the context of change point estimation, \citet{mallik} suggests
such a heuristic.
They discuss a random variable $x_t$, whose distribution is a function
of a continuous covariate $t$.
For $t<d^*$, $\EE x_t=\tau_0$, a constant; for $t>\dstar$, $\EE
x_t>\tau_0$.
They propose an estimate of $d_0$ based on p-values $p_t$
testing the hypotheses $H_{0t}:\EE x_t=\tau_0$.
They note that for $t<\dstar$, the null hypotheses are true, so
$p_t\sim U(0,1)$, and $\EE p_t =1/2$; when $t>\dstar$, the null hypotheses are false, and
the p-values converge in probability to zero.
That fact leads them to the following least-squares estimator for
$\dstar$:
\begin{equation*}
\dhatm\equiv arg\displaystyle\min_{d\in \mathbb{N}} \displaystyle\sum_{t\le d} (p_t -1/2)^2 +
\displaystyle\sum_{t>d} p_t^2.
\end{equation*}
In other words, the estimate $\dhatm$ is the point at which the
p-values cease behaving as p-values testing a true null, with mean
$1/2$, and instead are drawn from a distribution with a lower mean.
It turns out that an equivalent expression for $\dhatm$ is:
\begin{equation}\label{eq:mallikSimple}
\dhatm=argmax_d \displaystyle\sum_{t\le d} (p_t-1/4).
\end{equation}
\citet{mallik} shows that as $n_t$, the number of data points at each value
$t$, and the number of sampled values of $t$ increase, $\dhatm$
converges in probability to $\dstar$.

The same broad logic applies to any set of p-values from sequential tests:
$\dhatm=argmax_d \sum_{t\le d} (p_t-1/4)$ may be considered an
estimate of $\dstar$.
In the case of SSTs, for $d\le \dstar$, p-values $p_d$ are draws from
a $U(0,1)$ distribution, and hence have mean 1/2, and, as $n_d$ or $N$
increase, $p_d \rightarrow_p 0$ for $d>\dstar$.
Some differences in the details, though, lead to differences in
$\dhatm$'s behavior.
For instance:
\begin{lemma}
If indeed $p_d\rightarrow_p 0$ for $d>\dstar$, as $n_d$ or $N$
increase, then $\dhatm$ is assymptotically conservative:
$Pr(\dhatm>\dstar)\rightarrow 0$.
\end{lemma}
\begin{proof}
For each $d$, $Pr(p_d -1/4>0)\rightarrow 0$, implying that for all $d'$, $Pr(\sum_{\dstar <t\le d'}
(p_t-1/4)>0)\rightarrow 0$.
Therefore, for $\dstar<d\le D$, $Pr(\sum_{t\le d} (p_t-1/4)> \sum_{t\le
  \dstar} (p_t-1/4))\rightarrow 0$.
\end{proof}
That is, as sample size increases, the probability that $\dhatm$
suggests a model that violates assumption $\mathcal{A}$ decreases to
zero.
The same property holds for $\dalphaU$, with $\alpha>0$ fixed, for
the same reason.

On the other hand, even with an infinite sample $\dhatm$ may choose a
sub-optimal model, $\dhatm<\dstar$.
As sample size grows, the distribution of $p_d$, $d\le \dstar$ remains
stable at $U(0,1)$.
When $p_\dstar-1/4<0$, $\dhatm \neq \dstar$, since $\sum_{d\le
  \dstar-1} (p_d-1/4)>\sum_{d\le \dstar} (p_d-1/4)$.
Since $Pr(p_\dstar-1/4<0)=1/4$ regardless of sample size, $\dhatm$
will be conservative in large samples.
The difference between the SST case discussed here and the
change-point case in \citet{mallik} is that the latter case relies on
a continuous covariate that may be sampled from any point on the unit
interval, whereas in the SST case the choice set $d=1,2,\dots,D$ is
discrete and held fixed in the assymptotics.

In a way, $\dhatm$ is similar to $\dhatU_{0.25}$, the largest $d$ for
which $p_d>\alpha=0.25$, since both penalize p-values lower than
$0.25$.
However, for a given set of p-values, $\dhatm \le \dhatU_{0.25}$.
To see this, note that for all $d>\dhatU_{0.25}$, $p_d<0.25$, so every
summand $p_d-1/4$ after $\dhatU_{0.25}$ is negative.
Therefore, the maximum of $\sum_{i\le d} (p_i-1/4)$ must occur with $d
\le \dhatU_{0.25}$.
While $\dhatU_{0.25}$ and $\dhatm$ may often coincide, there are also
cases in which $\dhatm<\dhatU_{0.25}$.
This will happen when the maximum value of the random walk in
(\ref{eq:mallik}), occurs prior to $\dhatU_{0.25}-1$.
Then, $\dhatm$ will only equal $\dhatU_{0.25}$ if
$p_{\dhatU_{0.25}}-1/4>max_d\{ \sum_{i\le d} (p_i-1/4)\}-\sum_{i\le
  \dhatU_{0.25}-1} (p_i-1/4)$.

In general, the difference between $\dalphaU$ and $\dhatm$ will be
most pronounced when the distributions of p-values for $d>\dstar$ are
not monotonically decreasing in probability---in such a scenario, it
is most probable that an errent p-value for $d>>\dstar$ will be
greater than $\alpha$; one p-value determines $\dalphaU$, but
$\dhatm$ relies on the entire set of p-values.

\subsection{A More Flexible $\dhatm$}
In finite samples, p-values from tests of false null hypotheses will
not always be zero.
Similarly, many hypothesis tests are assymptotic and may not yield
uniformly-distributed p-values in finite samples.
Still, p-values from SSTs may exhibit something similar to the
dichotomous behavior that motivates $\dhatm$, in which p-values for
$d\le \dstar$ are distributed differently than p-values for
$d>\dstar$.
For this reason, \citet{mallik} suggested a more flexible estimate:
\begin{equation}
  \dhatmab \equiv arg\displaystyle\min_{\dhat\in \mathbb{N}; 0<b<a<1}
  \displaystyle\sum_{d\le \dhat} ( p_d
  -a)^2+\displaystyle\sum_{d>\dhat} (p_d-b)^2
\end{equation}
Like $\dhatm$, model selector $\dhatmab$ looks for behavior that
differs between p-values testing true and false null hypotheses.
Unlike $\dhatm$, it does not depend on theoretically established
distributions for these p-values, but searches over a grid for their
location parameters.
$\dhatmab$ will be more computationally expensive to compute than
$\dhatm$, but will often yield better results, especially in small
samples.

\begin{comment}
\subsection{Edge Testing}
Typically, the p-values from SSTs will be mutually correlated.
This will be particularly pronounced in situations analogous to RDD
bandwidth selection, in which SSTs are used to choose among nested
datasets.
In this situation, a p-value for choice $d$, $p_d$, is based on the
same data as the previous p-value, $p_{d-1}$, along with with sometimes only a
few extra cases.


In contrast to specification tests that researchers use to check
fully-specified models, and are designed to check the model as a
whole, SSTs are an explicit and planned part of the model selection
process.
That being the case, their focus should be on differences between
potential specifications, rather than on overall suitability.
We refer to the former as ``edge testing,'' since it focuses
hypothesis tests on edge cases, and the latter ``total testing.''

When decisions $d$ determine which data are included in the analysis,
as in RDD bandwidth selection, the choice between edge and total
testing is a choice between null hypotheses to test.
The edge null is:
\begin{equation}
\hedged: \mathcal{A} \text{is true for } i \in
\mathcal{i}_d
\end{equation}
whereas the total null is
\begin{equation}
\htotd: \mathcal{A} \text{is true for } i \in
\mathcal{I}_d
\end{equation}
where, as above,
$\mathcal{I}_d=\mathcal{i}_1\cup\dots\mathcal{i}_d$, all data
included in specification $d$.

For instance, in selecting a bandwidth for an RDD, as in
\citet{rocio}, researchers test for, say, equality of means of a
covariate $x$ between treated subjects, with running variable values
$R$ at one
side of the cutoff, and control subjects with $R$ on the other side.
Here $d$ indexes candidate bandwidths, $max |R-c|=bw_d$.
Then $\mathcal{i}_d=\{i: |R_i-c|=bw_d\}$ and $\mathcal{I}=\{i:
|R_i-c|\le bw_d\}$.
Therefore, $\htotd : \EE [x| 0<R-c\le bw_d]=\EE [x|-bw_d \le R-c <0]$
and $\hedged :\EE [x|R-c=bw_d]=\EE [x|
R-c=-bw_d]$.\footnote{These are simplifications of Assumption 4 in
  \citet{rocio}, which treats $x$ as fixed, not random.}
For the sake of demonstration, say $var(x)=\sigma^2$.
For $d \le \dstar$, $\EE [x ||R-c|= bw_d]=0$ but for $d>\dstar$
$\EE[x | R-c = bw_d]=\tau$ and $\EE [x|R-c =-bw_d]=-\tau$.
Further, say there are $n_d=n_0$ at each possible bandwidth $bw_d$.
For $d=\dstar+1$, testing $\htotd$ means comparing the means of two
samples of size $(\dstar+1)n_0$ , each with standard deviation
$\sqrt{\sigma^2+\tau^2(1-1/dstar)}$ and with means $\pm \tau/dstar$.
On the other hand, a test of $\hedged$ compares the means of two
smaller samples, each of size $n_0$, with standard deviation $\sigma$
and means $\pm \tau$.
As long as $\dstar>1$, the power of a t-test for $\hedged$ will be greater
than the power for $\htotd$,\footnote{The non-centrality parameter in
  the $\htotd$ test is
  $\frac{2\tau/\dstar}{\sqrt{\sigma^2+\tau^2(1-1/\dstar)}}\frac{\sqrt{(\dstar+1)n_0}}{\sqrt{2}}$
    and the non-centrality parameter in the $\hedged$ test is
    $\frac{2\tau}{\sigma}\frac{\sqrt{n_0}}{\sqrt{2}}$}
better allowing a the SST procedure to distinguish between $\dstar$
and $d$.

% The SST case in which a researcher uses SSTs to choose a model
% specification is analogous.
% For instance, \citet{ivanov} provide several examples of sequential
% tests to determine the order of an autoregressive process, and both
% edge and total tests are represented.
% The ``general to specific'' liklihood ratio test compares the
% determinants of estimated innovation covariance
% matrices of models assuming order $d$ and $d-1$, respectively.
% It tests the null hypothesis $\hedged$ that the order of the process
% is $d$ against the alternative that the order is $d-1$.
% They also discuss a ``Portmanteau'' Lagrange Multiplier test that
% tests $\htotd$, that there is no serial correlation in the residuals
% of a VAR($d$) model.
% In this case, the difference between edge and total tests lies in the
% alternative---an edge test compares model $d$ to model $d+1$, whereas
% a total test compares all candidate model to the same alternative.

\end{comment}

\section{A Simulation Study}\label{sec:simulation}
<<simulation>>=
#source('src/changePointSim.r')

beta=0.1

### not run:
## st <- simTot(1000,ns=c(10,100),beta=beta)
load('src/simulation26/simulation.RData')

facet <- facet2

nsim <- length(st[[1]])

mats <- lapply(st,function(x) do.call('rbind',lapply(x,function(xx) xx$full)))
eqLin <- mean(vapply(mats[1:3], function(x) mean(x[,'\\hat{d}_M']==x[,'\\hat{d}^{flex}_M'],na.rm=TRUE),1))
eqSin <- mean(vapply(mats[4:6], function(x) mean(x[,'\\hat{d}_M']==x[,'\\hat{d}^{flex}_M'],na.rm=TRUE),1))

@
This section will present a small simulation study to compare the
behavior of model selectors $\dalphaU$, $\dalphaB$,
$\dhatm$, and $\dhatmab$ in finite samples.

In the simulation, a researcher tests a sequence of hypotheses
\begin{equation}\label{eq:nullSim}
H_d: \EE [X|-d<r<0]=\EE [X|0<r<d]
\end{equation}
for a covariate $X$ and a sequencing variable $r\in [-30,30]$.
This might arise if the researcher wants to identify the largest
possible region around the origin in which $X$ is balanced around 0.

For every run of the simulation, $H_d$ is true for $d\le 10$, and,
with one exception, false for $d>10$.
In other words, the optimal choice for $d$---the largest $d$
satisfying (\ref{eq:nullSim})---is $\dstar=10$.
The simulation runs differ via two factors: the first factor is sample
size, the number of samples available at each value of $r$:
$n=\{10,50,100\}$.
The second factor concerns departures from $H_d$ for $d>10$, and is
illustrated in Figure \ref{fig:illustrateSim}: in simulations with
linear imbalance, departures from $H_d$ for $r>10$ are linear in $d$, with $\EE
[X|r]-\EE[X|-r]=2\beta(r-10)$, where $\beta=$\Sexpr{beta}.
In simulations with sinusoidal imbalance, departures from $H_d$ for
$r>10$ are sinusoidal.
In the latter case, $H_d$ is actually true for $d=30$, which will give
rise to radically different behavior between $\dalphaB$ and the other
model selectors.
\begin{figure}
<<illlustrateSim,fig.width=6,fig.height=4>>=
par(mfrow=c(1,2))
curve(ifelse(abs(x)>10,(x-sign(x)*10)*beta,0),-30,30,xlab='r',ylab='$\\mathbb{E}[X|r]$',
      main='Linear Imbalance')
curve(ifelse(abs(x)>10,sin((x-10*sign(x))*pi/10),0),-30,30,xlab='r',ylab='',
      main='Sinusoidal Imbalance')
@
\caption{Two designs for the simulation study.}
\label{fig:illustrateSim}
\end{figure}

In each dataset, the researcher tests $H_d$, $d=1,...,30$ via t-tests
and records the p-values.
These p-values, in turn, give rise to eight different choices for
$\hat{d}$: $\dalphaU$ and $\dalphaB$, with $\alpha=0.5,0.15,0.25$,
$\dhatm$, and $\dhatmab$.

Figure \ref{fig:linImb} shows the results of $nsim$ replications
under linear imbalance.
Table \ref{tab:linear} shows some statistics that may not be apparent
from the figure.
For all three sample sizes, the model selectors tended to select
samples that were too large.
However, as the sample size increased, and with it the power to reject
$H_b$ for smaller values of $b$, the performance of the selectors
improved.
$\dalphaU$ had the smallest
variance across the board, but at the
price of often choosing the larger datasets.
On the other extreme, $\dalphaB$ picked very small datasets---for all
three sample sizes, $\dhatB_{0.25}\le 3$ in over half of the
simulation runs.

$\dhatm$ and $\dhatmab$, on average, performed the best across
runs---their average choices were close to $\dstar=10$ in all three
runs.
However, they were more variable than $\dalphaU$, and occasionally
chose very small $\hat{d}$.
The flexible estimate $\dhatmab$ was less than or equal to $\dhatm$ in
every case, though the two estimates coincided about
\Sexpr{round(eqLin*100)}\% of the time.



\begin{figure}
<<linearImbalance,fig.width=6,fig.heigh=2,cache=TRUE>>=
source('src/simulation26/changePointSim.r')
facet(st,FALSE)
@
\caption{Simulation results for simulations featuring linear
  imbalance. The parameter $n$ controls the sample size at each value
  of $r$, from -30 to 30. Eight different model selectors are shown
  for each sample size via overlaied violin and box-plots}
\label{fig:linImb}
\end{figure}


<<linearTable,results='asis'>>=
ss <- sumStats(st)

#ss <- lapply(ss, function(x) x[,-grep('mse',colnames(ss))])
ss <- lapply(ss, function(x) x[,c('avg','sd','lessThan3','moreThan13')])


tab <- do.call("cbind",ss[1:3])
rownames(tab) <- paste0('$',c("\\dhatU_{0.05}","\\dhatU_{0.15}","\\dhatU_{0.25}","\\dhatB_{0.05}","\\dhatB_{0.15}","\\dhatB_{0.25}","\\dhatm","\\dhatmab"),'$')


colnames(tab) <- rep(c('avg','sd','$\\le 3$','$>13$'),3)

xtab <- xtable(tab,caption='The average and standard deviation of each selection rule $\\hat{d}$, as well as the proportions of runs each method selected $d\\le 3$ or $d>13$, for $n=10,50,100$ with linear covariate imbalance',label='tab:linear')
align(xtab) <- "r|cccc|cccc|cccc|"
digits(xtab) <- c(0,rep(c(1,1,2,2),3))
    addtorow <- list()
    addtorow$pos <- list(-1)
    addtorow$command <- c("& \\multicolumn{4}{c}{$n=10$}&\\multicolumn{4}{c}{$n=50$} &\\multicolumn{4}{c}{$n=100$}\\\\\n")
    print(xtab,sanitize.text.function=function(x) x,add.to.row=addtorow)

@

When departures from $H_d$ are not monotonic in $d$, as in the
sinusoidal simulation, differences between model selectors can be
starker.
Figure \ref{fig:sinImb} shows the results from that simulation.
Most striking is that $\dalphaU$ chooses the maximum $d=30$ rather
frequently.
This may not actually be a bad thing---indeed, the mean of the
covariate $X$ is equal between positive and negative $r$ when $d=30$.
However, depending on the application, large differences in the mean
of $X$ for particular values of $r$ may indicate other departures from
model assumptions.
Whether the behavior exhibited by $\dalphaU$, which occasionally
chooses the largest possible $d$, is preferable to that of $\dhatm$,
which rarely does, will depend on the specific data scenario.
That said, the results in Figure \ref{fig:sinImb} illustrate the
difference between a procedure like $\dalphaU$ whose decision may be driven by one
individual p-value, and one like $\dhatm$ which is driven by the
entire distribution of p-values.


\begin{figure}
<<sinusoidalImbalance,fig.width=6,fig.heigh=2,cache=TRUE>>=
facet(st,TRUE)
@
\caption{Simulation results for simulations featuring sinusoidal
  imbalance. The parameter $n$ controls the sample size at each value
  of $r$, from -30 to 30. Eight different model selectors are shown
  for each sample size via overlaied violin and box-plots}
\label{fig:linImb}
\end{figure}




\section{Two Data Examples}\label{sec:examples}

This section will present a more complete treatment of the two
examples from Section \ref{sec:exampleIntro}: choosing a bandwidth in
a regression discontinuity design, and choosing a lag order for an
autoregressive model.
The two examples correspond to two broad categories of
specification: selecting data to analyze and
selecting a model specification.

\subsection{SSTs in Regression Discontinuity Bandwidth Selection:
  Estimating the Effect of  Academic Probation on College GPAs}

At many universities, students who fail to achieve a minmum GPA are
put on academic probation (AP) \citep[See, e.g.][]{tovar2006academic}.
This provides them access to a set of resources designed to address
personal issues that may be hindering their  performance.
Perhaps more importanty, AP is a threat---students on AP who do not
improve are subject to disciplinary measures such as suspension.
\citet{lso} recognized that AP can form a regression discontinuity
design (RDD), in which treatment is a function of a ``running
variable'' with a pre-determined cutoff.
Specifically the treatment $Z$, students' AP status, is (almost) a
deterministic function of a ``running variable'' $R$, students'
grade-point-averages (GPAs).
Students with a GPA below a pre-determined cutoff, $R<c$, are put on AP.
That being the case, students with GPAs just below $c$ may be
comparable to students with GPAs just above $c$---comparing these two
sets of students allows researchers to estimate the effect of AP on
outcomes $Y$.
The challange becomes defining ``just above'' and ``just below''; SSTs
may be able to play a role here.

For example, \citet*{rocio} (CFT) suggests directly
comparing the outcomes of subjects with $R$ very close to $c$, say with
$R\in [c-bw,c+bw]$ for some bandwidth $bw>0$
To choose $bw$, CFT uses pre-treatment covariates $\bm{X}$, and
covariate balance tests range of candidate bandwidths.
For each possible $bw$, they test the hypothesis that the covariates
are balanced:
\begin{equation}\label{eq:covBal}
\bm{X}\independent Z| R\in[c-bw,c+bw]
\end{equation}
and choose the largest bandwidth in which (\ref{covBal}) cannot be
rejected.

Bandwidth selection for RDDs, and the role of covariate balance tests,
encompasses a growing literature.
As its name suggests, regression discontinuity typically relies on
regression modeling: the goal is to model $Y$ as a function of $R$ on
either side of $c$ to estimate the average treatment effect for
subjects with $R$ in an infinitesimally-small interval around the
cutoff $c$ \citep[See][]{imbensRD}.
In contrast, CFT dispenses with regression alltogether.
One popular way to ensure robustness to model misspecification is to
fit the regression models to a subset of the data with $R$ in a
window around $c$.
A number of methods exist to choose an optimal bandwidth $bw$---the width
of the window---that is both large enough to allow for precise effect
estimation but small enough to ensure robustness.
\citet{IK} suggest using non-parametric estimates of the curvature of
the regression function of $Y$ on $R$, combined with local linear
regression, to choose a $bw$ that minimizes mean-squared-error.
However, other authors have suggested choosing $bw$ (or an analogous quantity) based on
SSTs, including \citet{mattai}, which presents a Bayesian approach
analogous to CFT's, \citet{salesHansen}, which discusses the use of robust
regression models,  and \citet{angristWanna}, which proposes a method
to estimate effects for subjects with $R$ farther from $c$.
In the latter paper, SSTs do not test covariate balance, but the
irrelevence of $R$ conditional on covariates $X$, for subjects in a
given bandwidth.

This section will illustrate several approaches to SSTs in the context
of estimating the effect of AP for first year college students on
subsequent GPAs.
For the sake of simplicty, the discussion will be limited to CFT's
general approach to regression discontinuity designs; however, many of
the SST methods can be extended to other RDD analyses.
In their analysis, \citet{lso} considered a set of seven covariates:
students' high-school GPA (expressed in percentiles), age at college
metriculation, number of attempted credits, gender, native language
(English or other), birth place (North America or other) and
university campus (the university consisted of three campuses).
A version of Hotellings $T^2$ test that models
treatment assginment $Z$, and not $X$, as random \citep{hansenBowers}
is used to test balance.
The resulting p-values are plotted in the left panel of Figure \ref{fig:rddpvalues1}.
Various bandwidth selections are plotted as vertical lines in the
figure, and enumerated in Table \ref{tab:RDD}.

\begin{figure}
<<rdd,fig.width=6,fig.height=4,results='hide',cache=TRUE>>=

rddDat <- data.frame(bws=rep(c(0,bws),2),ps=c(0,psT,NA,psT1),incCovs=c(rep('All Covariates',length(bws)+1),rep('High School GPA',length(bws)+1)))
rddDat <- subset(rddDat,bws<=0.3)

dhatDat <- data.frame(value=c(multLine(c(0,bws)[windowsT+1],amount=0.002),
                          multLine(c(0,bws)[windowsT1+1],amount=0.002)),
                      Selector=c(names(windowsT),names(windowsT1)),incCovs=c(rep('All Covariates',length(windowsT)),rep('High School GPA',length(windowsT1))))



ggplot(rddDat,aes(bws,ps))+geom_point(size=4)+
    geom_vline(data=dhatDat,aes(xintercept=value,color=Selector),show.legend=TRUE)+ scale_colour_manual(values=cbbPalette)+
        facet_wrap(~incCovs,ncol=2)

## ggplot(merge(rddDat,dhatDat),aes(bws,ps))+geom_point()+geom_vline(aes(xintercept=value,color=Selector))+facet_wrap(~incCovs)

## par(mfrow=c(1,2))
## plot(c(0,bws),c(NA,psT),main='All Covariates',xlab='$bw$',ylab='p-value',ylim=c(0,1),xlim=c(0,0.3))
## abline(v=multLine(c(0,bws)[windowsT+1],amount=0.002),col=cols[1:length(windowsT)])
## legend('topright',legend=names(windowsT),col=cols[1:length(windowsT)],lty=1)

## plot(c(0,bws),c(NA,psT1),main='High School GPA',xlab='$bw$',ylab='p-value',ylim=c(0,1),xlim=c(0,0.3))
## abline(v=multLine(c(0,bws)[windowsT1+1],amount=0.002),col=cols[1:length(windowsT1)])
## legend('topright',legend=names(windowsT1),col=cols[1:length(windowsT1)],lty=1)

@
\caption{P-values from for balance in all seven covariates
  from the \citet{lso} analysis and only high school GPA,
  respectively. Vertical lines denote bandwidth choices using
  different criteria.}
\label{fig:rdpvalues1}
\end{figure}

<<rddTab,results='asis'>>=
tab <- cbind(d=windowsT,bw=c("N/A",bws)[windowsT+1],
             ATE=paste0(round(ates[1,],2),' (',round(ates[2,],3),')'),
             d=windowsT1,bw=c("N/A",bws)[windowsT1+1],
             ATE=paste0(round(ates2[1,],2),' (',round(ates2[2,],3),')'))
xtab <- xtable(tab,caption="Selected RDD bandwidths---'d' is the point in the sequence selected, and 'bw' is the bandwidth---using covariate balance tests for all the covariates in \\citet{lso} and only high school GPA, respectively, along with their assosicated estimates for the average treatment effect of academic probation (ATE), with standard errors.",label="tab:RDD")
align(xtab) <- "r|ccc|ccc|"
addtorow <- list()
addtorow$pos <- list(-1)
addtorow$command <- c("& \\multicolumn{3}{c}{All Covariates}&\\multicolumn{3}{c}{HS GPA}\\\\\n")
print(xtab,sanitize.text.function=function(x) x,add.to.row=addtorow)

@

The p-values from testing balance on all covariates do not follow a
monotonic pattern---they are near zero for very small bandwidths,
larger for bandwidths greater than \Sexpr{min(bws[psT>0.05])-0.01}, and close to zero again for
bandwidths greater than \Sexpr{max(bws[psT>0.05])}.
Here, as in the sinusoidal simulation in Section \ref{sec:simulation},
social scientists may disagree about appropriate bandwidth selelction.
On the one hand, covariates appear to be approximately balanced (or,
at least, there is no evidence to the contrary) for a range of
bandwidths.
On the other hand, the apparent covariate imbalance for small
bandwidths is worrying: perhaps it suggests deeper problems with this
design---suggesting either using regression to adjust for trends in
the running variable, or abandoning the design alltogether.

This ambivilance is reflected in the various bandwidth selectors.
$\dhatB_{0.05}$ and $\dhatB_{0.15}$ reject every possible bandwidth, while
both $\dhatm$ and $\dhat_M^{a,b}$ select the lowest possible
bandwidth of 0.
According to these methods, the CFT method is unsuitible for this
dataset.
However, the scattered large p-values at some bandwidths lead
 $\dalphaU$ to select larger bandwidthds.

To better illustrate differences between the window selection
strategies, we consider the covariate high school GPA alone.
Since the outcme of interest is itself a GPA, prior measures of GPA
are arguably the most relevant and important to control.
P-values from tests of balance in high school GPA are
displayed in the right panel of Figure \ref{fig:rdpvalues1}.
Fortunately for the illustration here, high school GPA may be
balanced for small bandwidths.
These p-values from are more nearly
monotonic, appearing roughly uniformly distributed for smaller
bandwidths and close to zero at high bandwidths.
The behavior of bandwidth selectors $\dhat$, shown with vertical lines
in the figure and in Table \ref{tab:RDD}, reflects this feature.
The first p-value below 0.05 occurs at a bandwidth of
\Sexpr{min(bws[psT1<0.05])}; accordingly, both $\dhatU_{0.05}$ and
$\dhatB_{0.05}$ select a bandwidth of \Sexpr{bws[windowsT1[1]]}.
On the other hand, the p-value of \Sexpr{psT1[2]} at \Sexpr{bws[2]}
caused $\dhatB_{0.15}$ to select the smallest possible bandwidth of 0.
The change-point selectors, $\dhatm$ and $\dhatmab$, along with
$\dhatU_{0.15}$, chose a bandwidth of \Sexpr{bws[windowsT1[6]]}.
In fact, a close inspection of Figure \ref{fig:rdpvalues1} reveals
that beginning at a bandwidth of 0.1, the p-values seem to be
decreasing---suggesting, perhaps, slight violations of the assumption
in (\ref{eq:covBal}).
$\dhatm$ and $\dhatmab$ chose a higher bandwidth than this due, in
part, to the high p-value at 0.15, which broke the trend, but
otherwise may have chosen an even smaller bandwidth.
In contradistiction, selectors such as $\dalphaU$ and $\dalphaB$ that
are based entirely on individual extreme p-values cannot account for
such patterns.

Table \ref{tab:RDD} also gives the estimated average treatment effect
for each bandwidth.
At very small bandwiths, the estimated effect is larger, though with a
large standard error as well.
The choice between bandwidths at the higher end does not make a large
difference in the estimated effects.

\subsection{Lag Order in AR($p$) Models: US Total Unemployment}

% \begin{figure}
% <<ts,fig.width=6,fig.height=4,results='hide'>>=

% source('src/timeSeries2.r')
% plot(y)
% @
% \caption{The United States total unemployment rate from 1890 to
%   2016. Data combined from \citet{urca} and \citet{bls}.}
% \label{fig:unemployment}
% \end{figure}

Figure \ref{fig:examples}B shows the natural logarithm of the United States total
unemployment rate from 1890 to 2016.
The data were combined from the ``Nelson \& Plosser extended data
set'' provided in the \texttt{urca} library in \texttt{R}
\citep{urca,Rcite}, which covers years 1890--1988, and a downloadable
dataset from the United States Bureau of Labor Statistics, itself
derived from the Current Population Survey, which covers years
1947--2015 \citep{blsData}.
The two datasets agree on the overlapping years.

Assume that the time series follows an ``AR($p$)'' model; that is,
\begin{equation}\label{eq:arp}
unemp_t=\mu + \displaystyle\sum_{i=1}^p \phi_i unemp_{t-i}+\epsilon_t
\end{equation}
where $\mu$ and $\{\phi_i\}_{i=1}^p$ are parameters to be estimated
and $\epsilon_t$ is white noise.
In this model, the unemployment in one year is a function of
unemployment rates in the previous $p$ years, but conditionally
independent of even earlier measurements.
More generally, we may write (\ref{eq:arp}) as
\begin{equation}\label{eq:arpinf}
unemp_t=\mu+\displaystyle\sum_{i=1}^\infty \phi_i
unemp_{t-i}+\epsilon_t
\end{equation}
with $\phi_i=0$ for $i>p$.

Having settled on model (\ref{eq:arp}), the analyst must choose $p$,
the lag order.
SSTs can be useful here \citep[e.g.][]{practitionersGuide,hatemi2009can,lutkepohl2005new}.
Consider the null hypothesis $H_p: \phi_i=0$ for all $i>p$;
a reasearcher could test a sequence of such null hypotheses, for a set
of plausible values of $p$, and choose the $p$ based on the results.
Of course, there are other options for choosing $p$, including
substantive theory or optimizing
information criteria, like AIC or BIC
(\citep{akaike1969fitting,schwarz1978estimating} though
\citealt{potscher1991effects} points out that differences in AIC or
BIC are essentially likelihood ratio test statistics).
In the absence of substantive theory, SSTs can assist a modler to
choose the smallest model that is still approximately correct---as
opposed to the model that maximizes predictive accuracy as measured
by, say, mean squared error.
A large literature surrounds this important question \citep[See,
e.g.][and the citations
therein]{mcquarrie1998regression,liew2004lag}. This section is not
meant as a complete treatment, or even an overview, of lag order
selection, but as an illustration of SSTs in a well-known area.

Figure \ref{fig:tspvalues1} gives the p-values from a sequence
likelihood ratio tests, as described in \citet[][Ch.1]{urca}, which
discussed a similar dataset.
For each candidate lag order $d$, the likelihood ratio test compares
twice the ratio of the log likelihoods of $AR(d+1)$ and $AR(d)$ models
to a $\chi^2_1$ distribution.
If the $AR(d+1)$ model fits much better than the $AR(d)$ model, a lag
order of $d$ may not be sufficient.
The p-values follow a stark pattern: for $d<5$, they are close to
zero, while for $d\ge5$, they appear roughly uniformly distributed.

Table \ref{tab:ts}, and vertical lines in Figure \ref{fig:tspvalues1},
show the lag order choices from $\dalphaU$, $\dalphaB$, $\dhatm$, and
$\dhatmab$, which are based on the p-values, and the lag orders that
minimize AIC and BIC, based directly on the models' likelihood and numbers
of parameters.
Here, smaller models are preferable to larger models, so $\dstar$ is
the \emph{smallest} acceptible value for $d$.
This is the opposite of the RDD case, which attempted to find the
\emph{largest} dataset on which to fit the model.

\Sexpr{stopifnot(length(unique(dhatsTSnm[c('alphaU15','dm','dmab','AIC','BIC')]))==1)}
\Sexpr{stopifnot(dhatsTSnm['alphaU05']<dhatsTSnm['dm'])}
\Sexpr{stopifnot(dhatsTSnm['alphaB05']==dhatsTSnm['alphaB15'])}
\Sexpr{stopifnot(dhatsTSnm['alphaB15']>dhatsTSnm['dm'])}

The change-point selectors $\dhatm$ and $\dhatmab$ both selected a
lag order of \Sexpr{dhatsTSnm['dm']}, consistant with the casual
observation that p-values for lags less than this value are very
small, while those greater appear approximately uniform.
Incidentally, the two information criteria considered, AIC and BIC,
agreed with this choice, as did $\dhatU_{0.15}$.
In contradistinction, $\dhatU_{0.05}$ chose a smaller lag order of
\Sexpr{dhatsTSnm['alphaU05']}, since the corresponding p-value of
\Sexpr{round(pvalsTS[dhatsTSnm['alphaU05']],3)} slightly exceeds the threshhold of 0.05.
\Sexpr{stopifnot(pvalsTS[min(dhatsTS)]>0.05)}
At the other extreme, the $\dalphaB$ selectors both chose very large
models with $d=$\Sexpr{dhatsTSnm['alphaB05']}, due to the presence of
of a small p-value of
\Sexpr{round(pvalsTS[dhatsTSnm['alphaB05']-1],3)} at
$d=$\Sexpr{dhatsTSnm['alphaB05']-1}.

This example illustrates how considering the entire distribution of
p-values, as $\dhatm$ does, can lead to better model selection than
considering only the small (as in $\dalphaB$) or large ($\dalphaU$)
values.


\begin{figure}
<<tsResults,fig.width=4,fig.height=4>>=

dhatsTS <- c(dhatAll(pvalsTS,backwards=TRUE),AIC=which.min(aics),BIC=which.min(bics))
par(mfrow=c(1,1),xpd=FALSE)

dhatDat <- data.frame(value=multLine(dhatsTS,amount=0.05),Selector=names(dhatsTS))
ggplot( mapping=aes(1:length(pvalsTS),pvalsTS))+geom_point(size=4)+labs(x="$d$",y="p-value")+
    geom_vline(data=dhatDat,aes(xintercept=value,color=Selector),show.legend=TRUE)

# plot(pvalsTS,pch=16,xlab='Lag Order',ylab='LR Test p-value',main='Unemployment $AR(p)$')
# abline(v=multLine(dhatsTS,amount=0.05),col=cols)
# par(xpd=TRUE)
# legend(21,1,legend=names(dhatsTS),
#        col=cols,lty=1)
@
\caption{P-values from sequential likelihood-ratio tests of model fit,
  comparing models $AR(d)$ with $AR(d+1)$ in the annual total US
  unemployment rate (logged) time series.}
\label{fig:tspvalues1}
\end{figure}

<<tsTab,results='asis'>>=
orders <- as.data.frame(as.integer(dhatsTS))
names(orders) <- 'Lag Order'
rownames(orders) <- names(dhatsTS)
print(xtable(orders,caption='Lag order selections for an $AR(d)$ model of the US unemployment time series.',
             label='tab:ts'),sanitize.text.function=function(x) x)
@

\section{Discussion}\label{sec:discussion}

\end{document}
