\documentclass[12pt]{article}

\usepackage{endfloat}
\usepackage{type1ec}
\usepackage{subcaption}
\usepackage{fullpage}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{multirow}
\usepackage{comment}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{setspace}
\usepackage{verbatim}
\usepackage{natbib}
\usepackage{bm}
\usepackage{pdflscape}
\usepackage{tikz}
\usepackage{xr}
\usepackage{hyperref}

\newcommand{\dalpha}{\hat{d}_\alpha}
\newcommand{\dstar}{d^*}
\newcommand{\ps}{\bm{p}_D}
\newcommand{\dhat}{\hat{d}}
\newcommand{\dhatm}{\hat{d}_M}
\newcommand{\hedged}{H_{0d}^{edge}}
\newcommand{\htoted}{H_{0d}^{tot}}

\doublespacing

\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newtheorem{conjecture}{Conjecture}
\newtheorem{ce}{Counter-Example}
%\newtheorem{ass}{Assumption}
\newtheorem{alg}{Algorithm}
%\newtheorem*{ass*}{Assumption}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}

\newenvironment{ass}[2][Assumption:]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2}.]}{\end{trivlist}}


<<functions>>=

dhatm <- function(ps){
    M <- cumsum(ps-1/4)
    which.max(M)
}

dhatAlpha <- function(ps,alpha){
    D <- length(ps)
    max(seq(D)[ps>=alpha],na.rm=TRUE)
}

ds <- function(ps,alphas=c(0.05,0.15,0.25)){
    dhat <- list()
    for(a in alphas){
        dhat[[paste0('alpha',a)]] <- dhatAlpha(ps,a)
    }
    dhat[['Mallik']] <- dhatm(ps)
    dhat
}

@

\title{Sequential Specification Tests to Choose a Model}

\author{Adam Sales}

\begin{document}

\maketitle

\section{Introduction}

\section{The Setup}

Say, in specifying a model, a research must choose from a discrete,
ordered, set of specifications, such as variables to include in a
linear model, bandwidths in a regression discontinuity design, or
calipers in a propensity-score matching design.
Denote the set of possible choices $d=1,2,...,D$.

The researcher wishes to choose the best model that satisfies a
particular testable assumption $\mathcal{A}$.
Assume that either $\mathcal{A}$ is false for all $d$, or that for
some $1\le \dstar\le D$, $\mathcal{A}$ is true for $d\le \dstar$ and false
for all $d>\dstar$.
Further assume that if $\dstar$ exists, it is the optimal choice---for
instance, the smallest model, or the biggest dataset, that satisfies
$\mathcal{A}$.
Finally, assume the researcher has chosen a valid, unbiased test of
$\mathcal{A}$ and caluculated p-values for each $d$:
$\bm{p}_D=p_1,...,p_d,...,p_D$.
The procedure here is to use $\bm{p}_D$ to choose a specification
$\hat{d}$ that is as large as possible without violating
$\mathcal{A}$.


A common choice for $d$ in this scenario relies on the logic of null
hypothesis testing: for a pre-specified $\alpha \in (0,1)$, let
\begin{equation*}
\dalpha \equiv max\{d : p_d>\alpha\}.
\end{equation*}
That is, $\dalpha$ is the largest value of $d$ for which the
null hypothesis that $\mathcal{A}$ is true for $d\le \dalpha$
cannot be rejected at level $\alpha$.
Although it may seem as though the multiplicity of tests invovled in
this procedure invalidates the null hypothesis framework, it turns out
that this is not the case: the ``stepwise intersection-union
pricipal'' \citet{berger1988, rosenbaum2008,hansen2015} insures that the
family-wise error rate is maintained.
That is, the probability of falsely rejecting the null---choosing
$\dalpha<\dstar$, is bounded by $\alpha$.
$\dalpha$ is the specification that would result from testing null
hypotheses backwards: for $d'=D,D-1,...,d,...,1$, test $H_{0d'}:$
$\mathcal{A}$ is true for $d\le d'$.
Then, stop testing at $d'=\dalpha -1$---the first $d'$ for which
$p_{d'} \ge \alpha$; reject all null hypotheses $H_{0d'}$ for which
$d'\ge \dalpha$, and fail to reject the rest.
This protects the family-wise error rate of $\alpha$ since rejecting
\emph{any} true null implies rejecting the first true null---a
probability $\alpha$ event.

A tempting alternative choice for $\hat{d}$, say $\hat{d}_{\tilde{\alpha}}$,
does not have this property.
The choice $\hat{d}_{\tilde{\alpha}}\equiv min\{d: p_d<\alpha\}-1$
selects $\hat{d}$ to be the largest value of $d$ before the first
significant p-value.
This is equavalent to the opposite procedure: start with the $d'=1$
and test sequentially for larger values of $d'$ until the first
rejection, at $\hat{d}_{\tilde{\alpha}}$, then stop; reject all null
hypotheses $H_{0d'}$ for $d'\ge \hat{d}_{\tilde{\alpha}}$ and fail to
reject the rest.
This procedure does not control family-wise error rates---it is likely
to reject more than $100\alpha$\% valid specifications.

This paper will focus on two data scenarios for SSTs.
In the first, the SSTs help determine which data are included in the
analysis.
For instance, choosing the bandwidth of a regression discontinuity
design, or choosing the parameters of a matching design.
In this scenario, each choice $d$ corresponds to rows in the dataset
that could be included in the analysis.
Formally, let $\mathcal{I}=\{1,...,N\}$, indices for $N$ candidate
cases to be fit in a model.
Then let $\mathcal{I}=\mathcal{i}_1\union\mathcal{i}_2\union...\union
\mathcal{i}_d \union \dots union \mathcal{i}_D$.
The choice of $\hat{d}$ means fitting the model to the dataset
including each of these subsets,
$\mathcal{I}_d=\mathcal{i}_1\union\dots\mathcal{i}_d$.
Note that the sets denoted with lower-case $\mathcal{i}_d$ are disjoint,
$\mathcal{i}_d \intersect \mathcal{i}_{d'}=\emptyset$,
those denoted with upper-case $\mathcal{I}_d$ are nested---$d>d'$
implies $\mathcal{I}_{d'}\subset \mathcal{I}_d$, and the full set of
indices, noted without a subscript, $\mathcal{I}=\mathcal{I}_D$.
Finally, let $n_d=|\mathcal{i}_d|$, where $|\cdot |$ denotes
cardinality, and $\bar{n_d}=|\mathcal{I}_d |$.

A second scenario applies when the dataset is fixed, but the model is
not.
Here, $d$ indexes a \emph{pre-specified} sequence of models.
For instance, if SSTs are used to determine which variables should be
included in a model, with the goal being smallest model that satisfies
$\mathcal{A}$.
Then let $\bm{X}$ denote the full set of variables, $\bm{x}_d$ denote
the set of variables that would be \emph{subtracted} in the $d^{th}$ step
of the sequence, and $\bm{X}_d=\bm{x}_{d+1} \union \dots \union \bm{x}_D$
denote the set of variables that would be included in the analysis,
were the analyst to choose $d$.
Note here that bigger values of $d$ correspond to smaller models.
In this scenario, the sample size is fixed at $N$.

\subsection{Model Selection and the Logic of Null Hypothesis Testing}
In order to avoid certain methodogical mistakes, it may be helpful to
clarify some of the conceptual distinctions between SSTs and
conventional null hypothesis tests (NHTs).
The logic of NHTs is familiar to anyone who has taken (and understood)
even the most basic college statistics course; nonetheless we restate
it here to distinguish it from the logic of SSTs.
Typically, researchers use NHTs to reject a null hypothesis that they
consider uninteresting---most of the time, that a model parameter is
equal to zero---and interpret rejection as evidence in favor of an
interesting alternative hypothesis.
NHTs cap the probability of a type-I error---falsely rejecting a true
null hypothesis---and, given that constraint, seek to minimize the
probability of a type-II error, failing to reject a false null
hypothesis.

SSTs reverse some of these elements; most importantly, the goal of
SSTs is to identify specifications in which an assumption
$\mathcal{A}$ is plausible, rather than to identify true alternative
hypothesis.
In the same vein, type-II errors are typically of more concern for
SSTs than for typical NHTs, and type-I errors are less problematic.
In fact, a type-II error from a specification test could lead a
researcher to fit a misspecified model, which in turn may inflate the
probability of a type-I error in her final outcome analysis.
For that reason, some methodologists recommend setting $\alpha$
substantially higher for specification tests than for NHTs in outcome
analyses.
Still, the hypothesis testing framework, in the case of pont null
hypotheses, does not allow a researcher to
fix the type-II error rate at a pre-specified value, and then optimize
the type-I error rate, though that might be ideal for
specificaton tests.

In fact, in continuous data models with continuous parameter spaces,
no hypothesis test can provide any evidence in favor of a point null
hypothesis.
For instance, take the common $H_0: \theta=0$, for some parameter
$\theta\in \mathbb{R}$.
In finite samples, for any type-I or type-II error rate, there will
always be some plausible alternative hypothesis $H_a
\theta=\epsilon\ne 0$.
Further, in these situations, finite sample estimates $\hat{\theta}$
will almost surely be non-zero.
This is important to state to avoid misinterpretations of SST
procedures as providing evidence, or showing, that an assumption
$\mathcal{A}$ is true for certain specificatons $d$.
A common Bayesian argument (e.g. \citealp[][p. 439]{kadanePrinciples};
\citealp{gelmanBlog}) states that, theoretically, nearly all null
hypotheses are false anyway---so testing them makes little sense.
In the case of specification tests, that means that an assumption
$\mathcal{A}$ can be assumed to be false for all $d$ without even
conducting a test; in other words, ``all models are wrong''
\citep[p. 2]{modelsWrong}.

``But some are useful.''
In practice there is much to be gained by considering assumptions such
as $\mathcal{A}$.
In this framework, it may indeed make sense to identify a set of
specifications $d$ for which $\mathcal{A}$ is plausible, or
approximately true, and
SSTs can be useful in this regard---as long as they are understood
correctly, and not as providing evidence \emph{for} $\mathcal{A}$.

In many scenarios the choice of $d$ involves a bias-variance tradeoff:
if $d>\dstar$, then $\mathcal{A}$ is false and the resulting analysis
will be biased.
On the other hand, a sub-optimal choice for $d$ often means a
high-variance estimate.
For instance, in the RDD bandwidth case, choosing $d>\dstar$ might mean
fitting a misspecified model to $Y$ and $R$, but choosing $d<<\dstar$
means discarding data that can boost precision.
Rather than choosing a criterion, such as mean-squared-error, that
balances bias and variance, the SST approach may be seen as an attempt to hold
bias at approximately zero, and minimize variance under that
constraint.
Granted, this is obviously an overly-optimistic take on model fitting;
still, SSTs hope to contrain bias to be approximately zero, and from there
minimize variance.


\subsection{More Reservations with Null Hypothesis Testing for Model
  Selection}

Applying a strict hypothesis-testing framework to SSTs for model
selection has some additional drawbacks.
First, it requires researchers to choose a test-level $\alpha$. While
using tuning parameters to mediate the bias-variance tradeoff is not
uncommon in statistics, the level $\alpha$ is a particularly hard
parameter to choose.

\citet{granger} poses an additional problems with the use of
hypothesis tests to choose a model: the need to specify a null
hypothesis. In their words (p. 179),
\begin{quote}
Whenever a hypothesis test is used to choose between two models, one
model must be selected as a null hypothesis. In most instances, this
is usually the more parsimonious model and typically a nested test is
applied. Often it is difficult to distinguish between the two models
because of data quality (multicol- linearity, near-identification, or
the models being very similar such as in testing for integration). In
such cases, the model chosen to be the null hypothesis is unfairly
favored.
\end{quote}
In other words, because of the structure of null hypothesis tests,
which constrain the type-I error rate, the null model is unfairly
favored.
In our terminology, $\hat{d}$ is likely to be too small, perhaps $\EE
\hat{d}<\dstar$.
However, such a bias (if it indeed exists) needn't doom SSTs---an
underestimated $\hat{d}$ is merely sub-optimal. In our setup, choosing
$\hat{d}$ to be too low will yield and inefficient, but still valid,
model.
Would that every statistical model were valid yet suboptimal!

More broadly, perhaps, one might argue that null hypothesis tests are
design to rule out hypotheses that are inconsistant with the data, not
to estimate parameters.
However, as \citet{hodgeslehmann} showed, these aims are not
contradictory---tests that rule out implausible hypothesis may also
point researchers towards the correct answer.

Moving from rejecting implausible specifications to estimating optimal
specifications requires a theory, or at least a reasonable heuristic.
The following section will suggest one.

\section{Finding the Change-Point}
In the context of change point estimation, \citet{mallik} suggests
such a heuristic.
They discuss a random variable $x_t$, whose distribution is a function
of a continuous covariate $t$.
For $t<d^*$, $\E x_t=\tau_0$, a constant; for $t>\dstar$, $\E
x_t>\tau_0$.
They propose an estimate of $d_0$ based on p-values $p_t$
testing the hypotheses $H_{0t}:\E x_t=\tau_0$.
They note that for $t<\dstar$, the null hypotheses are true, so
$p_t\sim U(0,1)$, and $\EE p_t =1/2$; when $t>\dstar$, the null hypotheses are false, and
the p-values converge in probability to zero.
That fact leads them to the following least-squares estimator for
$\dstar$:
\begin{equation*}
\dhatm\equiv argmin_d \displaystyle\sum_{t\le d} (p_t -1/2)^2 +
\displaystyle\sum_{t>d} p_t^2.
\end{equation*}
In other words, the estimate $\dhatm$ is the point at which the
p-values cease behaving as p-values testing a true null, with mean
$1/2$, and instead are drawn from a distribution with a lower mean.
It turns out that an equivalent expression for $\dhatm$ is:
\begin{equation}\label{eq:mallikSimple}
\dhatm=argmax_d \displaystyle\sum_{t\le d} (p_t-1/4).
\end{equation}
\citet{mallik} shows that as $n_t$, the number of data points at each value
$t$, and the number of sampled values of $t$ increase, $\dhatm$
converges in probability to $\dstar$.

The same broad logic applies to any set of p-values from sequential tests:
$\dhatm=argmax_d \sum_{t\le d} (p_t-1/4)$ may be considered an
estimate of $\dstar$.
In the case of SSTs, for $d\le \dstar$, p-values $p_d$ are draws from
a $U(0,1)$ distribution, and hence have mean 1/2, and, as $n_d$ or $N$
increase, $p_d \rightarrow_p 0$ for $d>\dstar$.
Some differences in the details, though, lead to differences in
$\dhatm$'s behavior.
For instance:
\begin{lemma}
If indeed $p_d\rightarrow_p 0$ for $d>\dstar$, as $n_d$ or $N$
increase, then $\dhatm$ is assymptotically conservative:
$Pr(\dhatm>\dstar)\rightarrow 0$.
\end{lemma}
\begin{proof}
For each $d$, $Pr(p_d -1/4>0)\rightarrow 0$, implying that for all $d'$, $Pr(\sum_{\dstar <t\le d'}
(p_t-1/4)>0)\rightarrow 0$.
Therefore, for $\dstar<d\le D$, $Pr(\sum_{t\le d} (p_t-1/4)> \sum_{t\le
  \dstar} (p_t-1/4))\rightarrow 0$.
\end{proof}
That is, as sample size increases, the probability that $\dhatm$
suggests a model that violates assumption $\mathcal{A}$ decreases to
zero.
The same property holds for $\dhat_\alpha$, with $\alpha>0$ fixed, for
the same reason.

On the other hand, even with an infinite sample $\dhatm$ may choose a
sub-optimal model, $\dhatm<\dstar$.
As sample size grows, the distribution of $p_d$, $d\le \dstar$ remains
stable at $U(0,1)$.
When $p_\dstar-1/4<0$, $\dhatm \neq \dstar$, since $\sum_{d\le
  \dstar-1} (p_d-1/4)>\sum_{d\le \dstar} (p_d-1/4)$.
Since $Pr(p_\dstar-1/4<0)=1/4$ regardless of sample size, $\dhatm$
will be conservative in large samples.
The difference between the SST case discussed here and the
change-point case in \citet{mallik} is that the latter case relies on
a continuous covariate that may be sampled from any point on the unit
interval, whereas in the SST case the choice set $d=1,2,\dots,D$ is
discrete and held fixed in the assymptotics.

In a way, $\dhatm$ is similar to $\dhat_{0.25}$, the largest $d$ for
which $p_d>\alpha=0.25$, since both penalize p-values lower than
$0.25$.
However, for a given set of p-values, $\dhatm \le \dhat_{0.25}$.
To see this, note that for all $d>\dhat_{0.25}$, $p_d<0.25$, so every
summand $p_d-1/4$ after $\dhat_{0.25}$ is negative.
Therefore, the maximum of $\sum_{i\le d} (p_i-1/4)$ must occur with $d
\le \dhat_{0.25}$.
While $\dhat_{0.25}$ and $\dhatm$ may often coincide, there are also
cases in which $\dhatm<\dhat_{0.25}$.
This will happen when the maximum value of the random walk in
(\ref{eq:mallik}), occurs prior to $\dhat_{0.25}-1$.
Then, $\dhatm$ will only equal $\dhat_{0.25}$ if
$p_{\dhat_{0.25}}-1/4>max_d\{ \sum_{i\le d} (p_i-1/4)\}-\sum_{i\le
  \dhat_{0.25}-1} (p_i-1/4)$.

In general, the difference between $\dhat_\alpha$ and $\dhatm$ will be
most pronounced when the distributions of p-values for $d>\dstar$ are
not monotonically decreasing in probability---in such a scenario, it
is most probable that an errent p-value for $d>>\dstar$ will be
greater than $\alpha$; one p-value determines $\dhat_\alpha$, but
$\dhatm$ relies on the entire set of p-values.

\section{Edge Testing}
Hypothesis tests will only be able to reject specifications that
violate assumption $\mathcal{A}$ if they have sufficient power.
One simple way to boost power in some scenarios is to focus hypothesis
tests on the parts of the data and model that is most likely to
violate the assumption---the difference between the analysis under $d$
and under $d-1$.

In contrast to specification tests that researchers use to check
fully-specified models, and are designed to check the model as a
whole, SSTs are an explicit and planned part of the model selection
process.
That being the case, their focus should be on differences between
potential specifications, rather than on overall suitability.
We refer to the former as ``edge testing,'' since it focuses
hypothesis tests on edge cases, and the latter ``total testing.''

When decisions $d$ determine which data are included in the analysis,
as in RDD bandwidth selection, the choice between edge and total
testing is a choice between null hypotheses to test.
The edge null is:
\begin{equation}
\hedged: \mathcal{A} \text{is true for } i \in
\mathcal{i}_d
\end{equation}
whereas the total null is
\begin{equation}
\htotd: \mathcal{A} \text{is true for } i \in
\mathcal{I}_d
\end{equation}
where, as above,
$\mathcal{I}_d=\mathcal{i}_1\union\dots\mathcal{i}_d$, all data
included in specification $d$.

For instance, in selecting a bandwidth for an RDD, as in
\citet{rocio}, researchers test for, say, equality of means of a
covariate $x$ between treated subjects, with running variable values
$R$ at one
side of the cutoff, and control subjects with $R$ on the other side.
Here $d$ indexes candidate bandwidths, $max |R-c|=bw_d$.
Then $\mathcal{i}_d=\{i: |R_i-c|=bw_d\}$ and $\mathcal{I}=\{i:
|R_i-c|\le bw_d\}$.
Therefore, $\htotd : \EE [x| 0<R-c\le bw_d]=\EE [x|-bw_d \le R-c <0]$
and $\hedged :\EE [x|R-c=bw_d]=\EE [x|
R-c=-bw_d]$.\footnote{These are simplifications of Assumption 4 in
  \citet{rocio}, which treats $x$ as fixed, not random.}
For the sake of demonstration, say $var(x)=\sigma^2$.
For $d \le \dstar$, $\EE [x ||R-c|= bw_d]=0$ but for $d>\dstar$
$\EE[x | R-c = bw_d]=\tau$ and $\EE [x|R-c =-bw_d]=-\tau$.
Further, say there are $n_d=n_0$ at each possible bandwidth $bw_d$.
For $d=\dstar+1$, testing $\htotd$ means comparing the means of two
samples of size $(\dstar+1)n_0$ , each with standard deviation
$\sqrt{\sigma^2+\tau^2(1-1/dstar)}$ and with means $\pm \tau/dstar$.
On the other hand, a test of $\hedged$ compares the means of two
smaller samples, each of size $n_0$, with standard deviation $\sigma$
and means $\pm \tau$.
As long as $\dstar>1$, the power of a t-test for $\hedged$ will be greater
than the power for $\htotd$,\footnote{The non-centrality parameter in
  the $\htotd$ test is
  $\frac{2\tau/\dstar}{\sqrt{\sigma^2+\tau^2(1-1/\dstar)}}\frac{\sqrt{(\dstar+1)n_0}}{\sqrt{2}}$
    and the non-centrality parameter in the $\hedged$ test is
    $\frac{2\tau}{\sigma}\frac{\sqrt{n_0}}{\sqrt{2}}$}
better allowing a the SST procedure to distinguish between $\dstar$
and $d$.

The SST case in which a researcher uses SSTs to choose a model
specification is analogous.
For instance, \citet{ivanov} provide several examples of sequential
tests to determine the order of an autoregressive process, and both
edge and total tests are represented.
The ``general to specific'' liklihood ratio test compares the
determinants of estimated innovation covariance
matrices of models assuming order $d$ and $d-1$, respectively.
It tests the null hypothesis $\hedged$ that the order of the process
is $d$ against the alternative that the order is $d-1$.
They also discuss a ``Portmanteau'' Lagrange Multiplier test that
tests $\htotd$, that there is no serial correlation in the residuals
of a VAR($d$) model.
In this case, the difference between edge and total tests lies in the
alternative---an edge test compares model $d$ to model $d+1$, whereas
a total test compares all candidate model to the same alternative.

\section{Two Data Examples}

To illustrate these ideas---edge testing, and the change point and
hypothesis testing approaches to selecting $d$---we will briefly
illustrate them with two data examples.
The two examples correspond to the two broad categories of
specification we have discussed: selecting data to analyze and
selecting a model specification.

\subsection{Regression Discontinuity to Estimate the Advantage of
  Incumbency}
Incumbency advantage---the extent to which electoral candidates
benefit from incumbency satus---is a longstanding problem in political
science \citet[See][and citations therein]{gelmanAndKing}.
While there are good theoretical reasons to suspect a strong
incumbency advantage, there are also serious sources of confounding
that must be accounted for.
A popular recent approach, beginning with \citet{leeElections} is to
rely on the Regression Discontinuity Design (RDD) inherent in
two-party majoritarian elections.
A candidate is elected to statewide office in the United States if and
only if his or her vote share exceeds the cutoff $c=$50\%.
In a given election, say time $t$, the incumbent's previous vote
share, at time $t-1$, is the
``running variable,'' which drives the assignment of
``treatment''---incumbency.
The outcome $Y$ is the incumbent's vote share at time $t$.
See, however, \citet{caughySekhon}, who offer a skeptical take.

As its name suggests, regression discontinuity typically relies on
regression modeling: the goal is to model $Y$ as a function of $R$ on
either side of $c$ to estimate the ``local average treatment effect,''
roughly the average treatment effect for subjects with $R$ in an
infinitesimally-small interval around the cutoff $c$
\citep[See][]{imbensRD}.
One popular way to ensure robustness to model misspecification is to
fit the regression models to a subset of the data with $R$ in a
window around $c$.
A number of methods exist to choose an optimal bandwidth $bw$---the width
of the window---that is both large enough to allow for precise effect
estimation but small enough to ensure robustness.
\citet{IK} suggest using non-parametric estimates of the curvature of
the regression function of $Y$ on $R$, combined with local linear
regression, to choose a $bw$ that minimizes mean-squared-error.
However, other authors have suggested choosing $bw$ (or an analogous quantity) based on
SSTs, including \citet{salesHansen}, which discusses the use of robust
regression models, \citet{mattai}, which presents a Bayesian approach, and
\citet{angristWanna}, which proposes a method to estimate effects for
subjects with $R$ farther from $c$.

Perhaps the simplest approach to RDDs is \citet{rocio}, which
dispenses with regression alltogether, opting instead to directly
compare the outcomes of subjects with $R$ very close to $c$, say with
$R\in c\pm bw$.
In this model, selection of $bw$ is not merely a matter of ensuring
robustness, but instead drives the indentification---subjects within
$bw$ of $c$, on either side, must resemble each other on observed and
unobserved pre-treatment covariates.
For that reason, \citet{rocio} was perhaps the first to suggest using
covariate balance tests---essentially tests for a treatment effect on
pre-treatment covariates---to choose $bw$.

%When there are several covariates measured, they suggest using the
%minimum of the p-values from balance tests of each of the covariates.

<<rdd>>=

### downloaded from http://www-personal.umich.edu/~cattaneo/software/rdlocrand/R/ 12/22/81




source('src/cft/rdlocrand_fun.R')
source('src/cft/rdrandinf.R')
source('src/cft/rdsensitivity.R')

rdDat <- read.csv('data/rdlocrand_senate.csv')


### code taken from rdlocrand-manual.pdf
### downloaded from http://www-personal.umich.edu/~cattaneo/software/rdlocrand/R/ 12/22/81

X  =  cbind(rdDat$presdemvoteshlag1, rdDat$population/1000000, rdDat$demvoteshlag1,
             rdDat$demvoteshlag2, rdDat$demwinprv1, rdDat$demwinprv2, rdDat$dopen,
             rdDat$dmidterm, rdDat$dpresdem)

colnames(X) =  c("DemPres Vote", "Population", "DemSen Vote t-1",
                  "DemSen Vote t-2", "DemSen Win t-1", "DemSen Win t-2",
                  "Open", "Midterm", "DemPres")

R = rdDat$demmv
Y = rdDat$demvoteshfor2
D = as.numeric(R>=0)


### first, make sure I didnt mess anything up in rocios code:
source('src/cft/rdwinselect.R')
rocioWinOrig <- rdwinselect(R,X,quietly=TRUE,seed=10,wmin=0.5,wstep=0.125,nwindows=50)

### now load in my modified code:
source('src/cft/rdwinselectModified.R')
## Rocio's p-values, window
rocioWin <- rdwinselect(R,X,quietly=TRUE,seed=10,wmin=0.5,wstep=0.125,nwindows=50)
stopifnot(all(vapply(1:length(rocioWin),function(i) all(rocioWin[[i]]==rocioWinOrig[[i]]),TRUE)))
rm(rocioWinOrig)
rocioWin$d <- which(rocioWin$results[,'W.length']==rocioWin$window[2])

## my preferred p-values
totPs <- rdwinselect(R,X,wmin=0.5,wstep=0.125,nwindows=50,approx=TRUE,
                     statistic='hotelling',quietly=TRUE,seed=10,edge=FALSE)$results

edgePs <- rdwinselect(R,X,wmin=0.5,obsstep=20,approx=TRUE,
                      statistic='hotelling',quietly=TRUE,seed=10,edge=TRUE)$results

### windows:
alphas <- c(0.05,0.15,0.25)
totWindows <- ds(totPs[,'p-value'],alphas=alphas)
edgeWindows <- ds(edgePs[,'p-value'],alphas=alphas)

par(mfrow=c(1,2))
plot(totPs[,'W.length'],totPs[,'p-value'],xlab='bw',ylab='p-value',main='Total Test P-values',pch=16,xlim=c(0,10))
abline(v=jitter(totPs[unlist(totWindows),'W.length'],amount=.2),col=2:5,lty=2)
legend('topright',
       legend=c('$\\hat{d}_{0.05}$','$\\hat{d}_{0.15}$',
           '$\\hat{d}_{0.25}$','$\\hat{d}_{Mallik}$'),
        col=2:5, lty=2)

plot(edgePs[,'W.length'],edgePs[,'p-value'],xlab='bw',ylab='p-value',main='Edge Test P-values',pch=16,xlim=c(0,10))
abline(v=jitter(edgePs[unlist(edgeWindows),'W.length'],amount=.2),col=2:5,lty=2)
legend('topright',
       legend=c('$\\hat{d}_{0.05}$','$\\hat{d}_{0.15}$',
           '$\\hat{d}_{0.25}$','$\\hat{d}_{Mallik}$'),
        col=2:5, lty=2)

estimates <- lapply(c(unlist(totWindows),unlist(edgeWindows)),
                    function(d) rdrandinf(Y,R,wl=-totPs[d,'W.length'],wr=totPs[d,'W.length'],
                                          ci=0.95,quietly=TRUE))

rocioEst <- rdrandinf(Y,R,wl=rocioWin$window[1],wr=rocioWin$window[2],quietly=TRUE)

results <- cbind(c('Total Tests',rep(NA,3),'Edge Tests',rep(NA,3)),
                 rep(c('$\\hat{d}_{0.05}$','$\\hat{d}_{0.15}$',
                       '$\\hat{d}_{0.25}$','$\\hat{d}_{Mallik}$'),2),
                 d=c(unlist(totWindows),unlist(edgeWindows)),
                 bw=round(totPs[c(unlist(totWindows),unlist(edgeWindows)),'W.length'],1),
                 n=rowSums(totPs[c(unlist(totWindows),unlist(edgeWindows)),c('Obs<c','Obs>=c')]),
                 Effect=round(vapply(estimates,function(x) x$obs.stat,1),2),
                 p.value=round(vapply(estimates,function(x) x$p.value,1),3))
results <- rbind(CFT=with(rocioWin,c(NA,NA,NA,round(window[2],1),
                     sum(results[which(results[,'W.length']==window[2]),c('Obs<c','Obs>=c')]),
                     round(rocioEst$obs.stat,2),round(rocioEst$p.value,3))),
                 results)


winNumL <- with(rocioWin,results[-1,'Obs<c']-results[-nrow(results),'Obs<c'])
winNumH <- with(rocioWin,results[-1,'Obs>=c']-results[-nrow(results),'Obs>=c'])
@

\citet{rocio} used their method to estimate incumbency advantage in
US Senate elections.\footnote{In fact, they estimated two different
  types of incumbency advantage; for purposes of illustration, we
  focus here on only their ``Design I.''}
They gathered a dataset from public sources including every senate
election from 1914, the first year senators were elected by state
popular votes, to 2010.
For each election, they also gathered nine covariates: the Democratic
share of the statewide presidential vote in the previous presidential
election, the state population, the Democratic share and indicators
for Democratic victory in each of the previous two
senate elections (each senator only stands for re-election in every
third election), an indicator for an open seat in the current
election, an indicator for midterm elections (i.e. no on presidential
electio years) and an indicator for a Democratic president.
Starting with a bandwidth of half a percentage point, and increasing
in increments of 1.25 percentage points, they tested for eqality of
means for elections either side of the cutoff---close Democratic wins
and close Republican wins---at a sequence of 77 bandwidths.
To accommodate all nine covariates, they chose, as $p_d$, the minimum
p-value across covariates.
Finally, they chose the $\hat{d}_{\tilde{\alpha}}^{th}$
bandiwdth---the largest bandwidth before the first p-value less than
the threshold 0.15.
This resulted in a bandwidth of \Sexpr{rocioWin[['window']][2]} percentage points, or a window of
(\Sexpr{round(c(50-rocioWin[['window']][2],2),50+rocioWin[['window']][2],2)}),
which included \Sexpr{rocioWin$results[rocioWin$d,'Obs<c']} Republican
incumbents and \Sexpr{rocioWin$results[rocioWin$d,'Obs>=c']}
Democratic incumbents.

We extended their analysis in order to illustrate the banwidth choices
$\dhat_\alpha$ and $\dhatm$, as well as the difference between total
and edge testing.
Rather than combine balance tests for individual covariates by taking
their minimum---a quantity that is not drawn from the uniform
distribution---we estimated omnibus p-values using the test in
\citet{hansenBowers}, a modification of Hotelling's $T^2$ test
designed specifically to test covariate balance in randomized trials.
Other than that modification, our total testing procedure was the same
as \citet{rocio}'s.
We tested covariate balance in a sequence of nested windows generated
by the sequence of bandwidths $bw_d=0.5, 0.625, 0.750,...,1$.

The edge testing procuedre required an additional modification:
a coarser division of the interval $(0,100)$ of electoral margins from
which candidate bandwidths are chosen.
In CFT's analysis, and in our ``total tests'' analysis, each new
bandwidth was 0.125 percentage points larger than the previous choice.
This coarsening added, on average, about \Sexpr{round(winNumL)}
subjects on each side of the cutoff.
In the edge testing


<<var>>=
library(vars)
@



\section{A Simulation Study}


\section{Discussion}

\end{document}